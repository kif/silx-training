{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a5eed5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Software Testing\n",
    "\n",
    "According to Wikipedia, software testing involves the execution of a software component to evaluate one or more properties of interest.\n",
    "\n",
    "In general, these properties indicate the extent to which the system under test:\n",
    "\n",
    "- meets the requirements that guided its design and development,\n",
    "- responds correctly to all kinds of inputs,\n",
    "- performs its functions within an acceptable time,\n",
    "- is sufficiently usable,\n",
    "- can be installed and run in its intended environments\n",
    "- achieves the general result its stakeholders desire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20ba4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Test?\n",
    "\n",
    "| Benefits                                 | Disadvantage                               |\n",
    "|:-----------------------------------------|:-------------------------------------------|\n",
    "| Find problems early                      | Extra work (to write and execute)          |\n",
    "| Globally reduce the cost of development  | Maintain test environments                 |\n",
    "| Safer to make changes to the code        | More difficult to change the code behavior |\n",
    "| Improve the software design              | Does not mean it is bug-free               |\n",
    "| It is part of documentation and examples | &nbsp;                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e82895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Kind of Tests?\n",
    "\n",
    "- <span style=\"color:#ee5aa0\">**Unit tests**</span>: Tests independent pieces of code.\n",
    "- <span style=\"color:#19bdcd\">**Integration tests**</span>: Tests components together.\n",
    "- <span style=\"color:#1aac5b\">**System tests**</span>: Tests a completely integrated application.\n",
    "- <span style=\"color:#b8b800\">**Acceptance tests**</span>: Tests the application with the customer.\n",
    "- [**And many more**](https://en.wikipedia.org/wiki/Software_testing).\n",
    "\n",
    "<img src=\"img/test-kind.svg\" style=\"height:45%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81478698",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing Frameworks\n",
    "\n",
    "- [unittest](https://docs.python.org/3/library/unittest.html): the default Python module for testing.\n",
    "- [pytest](https://docs.pytest.org): an alternative testing framework that makes writing tests more enjoyable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56b95d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Why do Developers Like `pytest`\n",
    "\n",
    "- Simple tests are simple to write in `pytest`.\n",
    "- Complex tests are still simple to write.\n",
    "- Tests are easy to read.\n",
    "- Tests are easy to read. (So important it's listed twice.)\n",
    "- You can get started in seconds.\n",
    "- You use `assert` to fail a test, not things like `self.assertEqual()` or `self.assertLessThan()`. Just assert.\n",
    "- You can use `pytest` to run tests written for `unittest` or `nose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a2302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The `ipytest` package is only needed to run tests in the Jupyter Notebook.\n",
    "# Most of the time, you will run `pytest` from the command line, as shown later.\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "# Configure ipytest with reasonable defaults (https://github.com/chmp/ipytest#reference).\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e54deb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def test_passing():\n",
    "    assert [-1, 1] == [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00084882",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ipytest.clean_tests()\n",
    "\n",
    "def test_failing():\n",
    "    assert [-1, 1] == [-1, 0]\n",
    "\n",
    "ipytest.run(\"-vv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3c66a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naming Conventions\n",
    "\n",
    "In order to make your tests discoverable by `pytest` they need to follow a predefined naming convention, detailed in the [official documentation](https://docs.pytest.org/en/stable/goodpractices.html#test-discovery). \n",
    "\n",
    "- Test files should be named test_**something**.py or **something**_test.py. \n",
    "- Test methods and functions should be named test_**something**.\n",
    "- Test classes should be named Test**Something**.\n",
    "\n",
    "There are ways to alter these discovery rules, but for the majority of cases is better to follow the default naming convention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fd764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Code Under Test\n",
    "\n",
    "Test the `polynom` function provided in the `pypolynom` sample project. \n",
    "\n",
    "It solves the equation $ax^2 + bx + c = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdc29c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def polynom(a, b, c):\n",
    "    \"\"\"Calculate the roots of a quadratic equation.\"\"\"\n",
    "    if a == 0:\n",
    "        # Not a second-degree polynomial.\n",
    "        raise ValueError(\"Not a quadratic equation if a = 0\")\n",
    "    delta = (b ** 2.0) - 4.0 * a * c\n",
    "    solutions = []\n",
    "    if delta > 0:\n",
    "        solutions.append((-b + (delta ** 0.5)) / (2.0 * a))\n",
    "        solutions.append((-b - (delta ** 0.5)) / (2.0 * a))\n",
    "    elif delta == 0:\n",
    "        solutions.append(-b / (2.0 * a))\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbb002",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "def test_no_roots():\n",
    "    result = polynom(2, 0, 1)\n",
    "    assert result == []\n",
    "\n",
    "\n",
    "def test_one_root():\n",
    "    result = polynom(2, 0, 0)\n",
    "    assert result == [0.0]\n",
    "\n",
    "\n",
    "def test_two_roots():\n",
    "    result = polynom(4, 0, -4)\n",
    "    assert result == [1.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e7aa0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The point of the tests is to check your understanding of how the code works, and to document that knowledge for someone else or even for a future self."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d483d592",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assertion Rewriting in `pytest`\n",
    "\n",
    "Rewritten assert statements put introspection information into the assertion failure message. \n",
    "\n",
    "In other words, the error messages you will get when an assertion fails in your tests will be more detailed than when an assertion fails in *regular* Python code.\n",
    "\n",
    "By default, assertion rewriting is limited to test modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769fa7a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test_two_roots():\n",
    "    result = polynom(2, 0, 0)\n",
    "    assert result == [1.0, -1.0]\n",
    "\n",
    "\n",
    "test_two_roots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4a89c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "def test_two_roots():\n",
    "    result = polynom(2, 0, 0)\n",
    "    assert result == [1.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81899007",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When do Tests Fail?\n",
    "\n",
    "- An `assert` statement fails, which results in an `AssertionError` exception being raised.\n",
    "- The code under test raises an exception that is not caught by the code under test or the test code.\n",
    "- The test code raises an exception.\n",
    "- The test code calls `pytest.fail()`, which in turn raises an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ed61b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test for Expected Exceptions\n",
    "\n",
    "We've looked at how uncaught exceptions can cause a test to fail. But what if the code you are testing is supposed to raise an exception? How do you test for that? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "def test_for_missing_arguments():\n",
    "    with pytest.raises(ValueError):\n",
    "        polynom(0, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a36d27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structuring Test Functions\n",
    "\n",
    "It is recommended to keep assertions at the end of test functions. This is such a common recommendation that it has at least two names: **Arrange-Act-Assert** and **Given-When-Then**. The structure helps to keep test functions organized and focused on testing one behavior.\n",
    "\n",
    "- **Arrange**, or set up, the conditions for the test.\n",
    "- **Act** by calling some function or method.\n",
    "- **Assert** that some end condition is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234e7c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "def test_number_of_roots():\n",
    "\n",
    "    # Arrange\n",
    "    a, b, c = 2, 0, 1\n",
    "\n",
    "    # Act\n",
    "    roots = polynom(a, b, c)\n",
    "\n",
    "    # Assert\n",
    "    assert len(roots) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e331a87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping Tests using Classes\n",
    "\n",
    "Using classes can help with providing some logical grouping of tests and can also be used to inherit helper methods. But don't get too fancy with this, as it might confuse others or even a future version of yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92d588",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "class TestQuadraticFunction:\n",
    "    def test_no_roots(self):\n",
    "        result = polynom(2, 0, 1)\n",
    "        assert result == []\n",
    "\n",
    "    def test_one_root(self):\n",
    "        result = polynom(2, 0, 0)\n",
    "        assert result == [0.0]\n",
    "\n",
    "    def test_two_roots(self):\n",
    "        result = polynom(4, 0, -4)\n",
    "        assert result == [1.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bc0a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where to Put the Tests?\n",
    "\n",
    "Separate tests from the source code:\n",
    "\n",
    "- Run the test from the command line.\n",
    "- Separate tests and code distributing.\n",
    "- [...](https://docs.python.org/3/library/unittest.html#organizing-test-code)\n",
    "\n",
    "Folder structure:\n",
    "\n",
    "- In a separate `test/` folder.\n",
    "- In `test` sub-packages in each Python package/sub-package,\n",
    "  so that tests remain close to the source code.\n",
    "  Tests are installed with the package and can be run from the installation.\n",
    "- A `test_*.py` for each module and script (and more if needed).\n",
    "- Consider separating tests that are long to run from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef1dda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "```\n",
    "project/\n",
    "  - setup.cfg\n",
    "  - package/\n",
    "    - __init__.py\n",
    "    - module1.py\n",
    "    - test/\n",
    "      - __init__.py\n",
    "      - test_module1.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a72bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running `pytest`\n",
    "\n",
    "- pytest: With no arguments, pytest searches the local directory and subdirectories for tests.\n",
    "- pytest **filename**: Runs the tests in one file.\n",
    "- pytest **filename** **filename** ...: Runs the tests in multiple named files.\n",
    "- pytest **dirname**: Starts in a particular directory (or more than one) and recursively searches for tests. \n",
    "- pytest **filename::test_something**: Runs a specific test function within a test file.\n",
    "- pytest **filename::TestClass::test_something**: Runs a specific test function within a test class within a test file.\n",
    "- pytest -k **pattern**: Runs the tests matching a name pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9deefc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ipytest.run(\"-k (root or roots) and not two\", \"-v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d98565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixtures\n",
    "\n",
    "Fixtures are functions that are run by `pytest` before (and sometimes after) the actual test functions. \n",
    "\n",
    "You can use fixtures to get a data set for the tests to work on or to prepare the system into a known state before running a test. Each test that depends on a fixture must explicitly accept that fixture as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd6e19",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "@pytest.fixture\n",
    "def some_data():\n",
    "    \"\"\"Return answer to the ultimate question.\"\"\"\n",
    "    return 42\n",
    "\n",
    "\n",
    "def test_some_data(some_data):\n",
    "    \"\"\"Use fixture return value in a test.\"\"\"\n",
    "    assert some_data == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03760891",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Using Fixtures for Setup and Teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3528468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipytest.clean_tests()\n",
    "\n",
    "\n",
    "@pytest.fixture()\n",
    "def image_data():\n",
    "    with open(\"img/test-pyramid.svg\") as fp:\n",
    "        print(\"Open file.\")\n",
    "        yield fp.read()\n",
    "        print(\"Close file.\")\n",
    "\n",
    "\n",
    "def test_version(image_data):\n",
    "    assert 'version=\"1.0\"' in image_data\n",
    "\n",
    "\n",
    "def test_encoding(image_data):\n",
    "    assert \"UTF-16\" in image_data\n",
    "\n",
    "\n",
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774dc4c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "- Specifying the fixture scope: when are the setup and teardown running relative to running all the test functions using the fixture `@pytest.fixture(scope=\"module\")`.\n",
    "- Fixtures can return data using `return` or `yield`.\n",
    "- Sharing fixtures through `conftest.py` (is read automatically by `pytest`, no need to import it).\n",
    "- Listing all fixtures: `pytest --fixtures`.\n",
    "- See the order of executions: `pytest --setup-show`.\n",
    "- Renaming fixtures: `name=\"new_fixture_name\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89210fd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Builtin Fixtures \n",
    "\n",
    "Some of the fixtures that are included in `pytest` are:\n",
    "\n",
    "- `tmp_path`: A function-scope fixture that creates a temporary directory and returns a `pathlib.Path` object pointing to it.\n",
    "- `tmp_path_factory`: A session-scope fixture returns a `TempPathFactory` object. This object has a `mktemp()` method that returns `pathlib.Path` objects.\n",
    "- `capsys`: A function-scope fixture that enables the capturing of writes to `stdout` and `stderr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f902d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def test_tmp_path(tmp_path):\n",
    "    \"\"\"Create a file in a temporary directory.\"\"\"\n",
    "    file_path = tmp_path / \"test.txt\"\n",
    "    file_path.write_text(\"Hello, world!\")\n",
    "    assert file_path.read_text() == \"Hello, world!\"\n",
    "\n",
    "\n",
    "def test_tmp_path_factory(tmp_path_factory):\n",
    "    \"\"\"Create a file in a temporary directory.\"\"\"\n",
    "    path = tmp_path_factory.mktemp(\"sub\")\n",
    "    file_path = path / \"test.txt\"\n",
    "    file_path.write_text(\"Hello, world!\")\n",
    "    assert file_path.read_text() == \"Hello, world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4a40b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "import sys\n",
    "\n",
    "def test_output(capsys):\n",
    "    print(\"Hello\")\n",
    "    sys.stderr.write(\"world\\n\")\n",
    "    captured = capsys.readouterr()\n",
    "    assert captured.out == \"Hello\\n\"\n",
    "    assert captured.err == \"world\\n\"\n",
    "    print(\"Next\")\n",
    "    captured = capsys.readouterr()\n",
    "    assert captured.out == \"Next\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f627a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ipytest.clean_tests()\n",
    "\n",
    "def test_normal(): \n",
    "    print(\"Normal print.\")\n",
    "\n",
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7e0e3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "- Add `--capture-no` to disable capturing of `stdout` and `stderr`.\n",
    "- `capfd`, `capfdbinary`, `capsysbinary`: Variants of capsys that work with file descriptors and/or binary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab33235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "def test_disabled(capsys): \n",
    "    with capsys.disabled():\n",
    "        print(\"Can you see this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc4a1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametrized Tests\n",
    "\n",
    "How to turn one test function into many test cases to test more thoroughly with less work?\n",
    "\n",
    "Parametrized testing is a way to send multiple sets of data through the same test and have `pytest` report if any of the sets failed.\n",
    "\n",
    "- Parametrizing functions.\n",
    "- Parametrizing fixtures.\n",
    "- Using a hook function called `pytest_generate_tests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c8eef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"coefs, roots\", [([2, 0, 1], []), ([2, 0, 0], [0.0]), ([4, 0, -4], [1.0, -1.0])]\n",
    ")\n",
    "def test_quadratic_function(coefs, roots):\n",
    "    assert polynom(*coefs) == roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ff254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "@pytest.fixture(params=[([2, 0, 1], []), ([2, 0, 0], [0.0]), ([4, 0, -4], [1.0, -1.0])])\n",
    "def quadratic_data(request):\n",
    "    return request.param\n",
    "\n",
    "\n",
    "def test_quadratic_function(quadratic_data):\n",
    "    coefs, roots = quadratic_data\n",
    "    assert polynom(*coefs) == roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2ed50",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "Fixture parametrization has the benefit of having a fixture run for each set of parameters. This is useful if you have setup or teardown code that needs to run for each test case. Maybe a different database connection, or different contents of a file, or whatever.\n",
    "\n",
    "It also has the benefit of many test functions being able to run with the same set of parameters. All tests that use the start_state fixture will all be called three times, once for each start state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08d4de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimate tests' quality\n",
    "\n",
    "Using [`coverage`](https://coverage.readthedocs.org) to gather coverage statistics while running the tests (`pip install coverage`).\n",
    "\n",
    "```bash\n",
    "$ python -m coverage run -m pytest\n",
    "$ python -m coverage report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b28c3",
   "metadata": {},
   "source": [
    "```\n",
    "Name                              Stmts   Miss  Cover\n",
    "-----------------------------------------------------\n",
    "pypolynom/__init__.py                 0      0   100%\n",
    "pypolynom/mathutil.py                 8      0   100%\n",
    "pypolynom/polynom.py                 16      0   100%\n",
    "pypolynom/test/__init__.py            0      0   100%\n",
    "pypolynom/test/test_mathutil.py      17      0   100%\n",
    "pypolynom/test/test_polynom.py       25      0   100%\n",
    "-----------------------------------------------------\n",
    "TOTAL                                66      0   100%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61058d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration\n",
    "\n",
    "Automatically test the software for each change applied to the source code.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Be aware of problems early.\n",
    "    - Before merging a change on the code.\n",
    "    - On third-party library update (sometimes before the release).\n",
    "    - Reduce the cost in case of problem.\n",
    "- Improve contributions and team work.\n",
    "\n",
    "Costs:\n",
    "\n",
    "- Set up and maintenance.\n",
    "- Testing needs to be automated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81c813",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous Integration\n",
    "\n",
    "- [AppVeyor](http://www.appveyor.com/), [GitHub Actions](https://docs.github.com/en/actions), [GitLab CI](https://gitlab.esrf.fr).\n",
    "- A `.yml` file describing environment, build, installation, test process.\n",
    "\n",
    "<img src=\"img/ci-workflow.svg\" style=\"width:40%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8b3c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Configuration\n",
    "\n",
    "A minimal configuration file for testing with GitLab CI. (.gitlab-ci.yml)\n",
    "\n",
    "```yaml\n",
    "# The official Python language Docker image.\n",
    "image: python:latest\n",
    "\n",
    "before_script:\n",
    "  - python -V  \n",
    "  - pip install virtualenv\n",
    "  - virtualenv venv\n",
    "  - source venv/bin/activate\n",
    "\n",
    "test:\n",
    "  script:\n",
    "    - pip install pytest\n",
    "    - pytest\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "5c9bf53eb292b7eb3096606b550d2614b7a7869ac2a226817363709a47dc75e3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "rise": {
   "height": "100%"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
