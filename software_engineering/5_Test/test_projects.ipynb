{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This have to be executed before running the slides\n",
    "# Monkey patch unittest to be able to run it in the notebook\n",
    "\n",
    "import unittest\n",
    "\n",
    "def apply_jupyter_patch():\n",
    "    default_unittest_main = unittest.main\n",
    "\n",
    "    def jupyter_unittest_main(**kwargs):\n",
    "        if \"argv\" not in kwargs:\n",
    "            kwargs[\"argv\"] = ['ignored']\n",
    "        kwargs[\"exit\"] = False\n",
    "        default_unittest_main(**kwargs)\n",
    "\n",
    "    unittest.main = jupyter_unittest_main\n",
    "\n",
    "apply_jupyter_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Testing\n",
    "=======\n",
    "\n",
    "- Introduction\n",
    "- Python `unittest` module\n",
    "- Estimate tests' quality\n",
    "- Continuous integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is it?\n",
    "\n",
    "- A task consisting of checking that the **program** is working as expected\n",
    "- Manually written **tests** which can be automatically executed\n",
    "- Different methodologies: Always and before anything else\n",
    "\n",
    "<img src=\"img/ttd-workflow.svg\" style=\"width:40%;margin-left:auto;margin-right:auto;padding:2em;\">\n",
    "\n",
    "- Harry J.W. Percival (2014). [Test-Driven Development with Python. O'Reilly](https://www.oreilly.com/library/view/test-driven-development-with/9781449365141/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "- Test inject input to the program, and check output\n",
    "- Answer valid or not\n",
    "- For maintenance: Reproduce bug in a test, then fix it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why testing?\n",
    "\n",
    "- Validate the code to the specifications\n",
    "- Find problems early\n",
    "- Facilitates change\n",
    "- Documentation\n",
    "- Code quality\n",
    "\n",
    "  - Better design\n",
    "  - Simplifies integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why not?\n",
    "\n",
    "- Extra work\n",
    "- Not perfect\n",
    "- Extra maintenance\n",
    "\n",
    "  - More difficult to refactoring\n",
    "  - Maintain environments\n",
    "\n",
    "- Delays integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "- 30% percent of the time of a project\n",
    "- Having the structure set-up for testing encourages writing tests.\n",
    "- Then... let's talk about the structure :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What kinds of tests?\n",
    "\n",
    "- <span style=\"color:#ee5aa0\">**Unit tests**</span>: Tests independant pieces of code\n",
    "- <span style=\"color:#19bdcd\">**Integration tests**</span>: Tests components together\n",
    "- <span style=\"color:#1aac5b\">**System tests**</span>: Tests a completely integrated application\n",
    "- <span style=\"color:#b8b800\">**Acceptance tests**</span>: Tests the application with selected users (can't be automated)\n",
    "\n",
    "<img src=\"img/test-kind.svg\" style=\"width:40%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "The test pyramid is a concept developed by Mike Cohn, described in his book \"Succeeding with Agile\"\n",
    "\n",
    "- Unit tests (dev point of view, fast, low cost)\n",
    "- Integration tests\n",
    "- Functional tests (user point of view, but slow, and expensive)\n",
    "\n",
    "- Cost: unit << integration << system\n",
    "- Fast to execute: unit >> integration >> system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where to put the tests?\n",
    "\n",
    "Separate tests from the source code:\n",
    "\n",
    "- Run the test from the command line.\n",
    "- Separate test code when distributing.\n",
    "- [...](https://docs.python.org/3/library/unittest.html#organizing-test-code)\n",
    "\n",
    "Folder structure:\n",
    "\n",
    "- In a separate `test/` folder.\n",
    "- In `test` sub-packages in each Python package/sub-package,\n",
    "  so that tests remain close to the source code.\n",
    "  Tests are installed with the package and can be run from the installation.\n",
    "- A `test_*.py` for each module and script (an more if needed).\n",
    "- Consider separating tests that are long to run from the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where to put the tests?\n",
    "\n",
    "- `project`\n",
    "  - `setup.py`\n",
    "  - `run_tests.py`\n",
    "  - `package/`\n",
    "    - `__init__.py`\n",
    "    - `module1.py`\n",
    "    - `test/`\n",
    "      - `__init__.py`\n",
    "      - `test_module1.py`\n",
    "    - `subpackage/`\n",
    "      - `__init__.py`\n",
    "      - `module1.py`\n",
    "      - `module2.py`\n",
    "      - `test/`\n",
    "        - `__init__.py`\n",
    "        - `test_module1.py`\n",
    "        - `test_module2.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest` Python module\n",
    "\n",
    "[unittest](https://docs.python.org/3/library/unittest.html) is the default Python module for testing.\n",
    "\n",
    "It provides features to:\n",
    "\n",
    "- Write tests\n",
    "- Discover tests\n",
    "- Run those tests\n",
    "\n",
    "Other frameworks exists:\n",
    "\n",
    "- [pytest](http://pytest.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Write and run tests\n",
    "\n",
    "The classe `unittest.TestCase` is the base class for writting tests for\n",
    "Python code.\n",
    "\n",
    "The function `unittest.main()` provides a command line interface to\n",
    "discover and run the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMyTestCase(unittest.TestCase):\n",
    "\n",
    "    def test_my_test(self):\n",
    "        # Code to test\n",
    "        a = round(3.1415)\n",
    "        # Expected result\n",
    "        b = 3\n",
    "        self.assertEqual(a, b, msg=\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assertion functions\n",
    "\n",
    "- Argument(s) to compare/evaluate.\n",
    "- An additional error message.\n",
    "\n",
    "- `assertEqual(a, b)` checks that `a == b`\n",
    "- `assertNotEqual(a, b)` checks that `a != b`\n",
    "- `assertTrue(x)` checks that `bool(x) is True`\n",
    "- `assertFalse(x)`checks that `bool(x) is False`\n",
    "- `assertIs(a, b)` checks that `a is b`\n",
    "- `assertIsNone(x)` checks that `x is None`\n",
    "- `assertIn(a, b)` checks that `a in b`\n",
    "- `assertIsInstance(a, b)` checks that `isinstance(a, b)`\n",
    "\n",
    "There's more, see [unittest TestCase documentation](https://docs.python.org/3/library/unittest.html#unittest.TestCase>)\n",
    "or [Numpy testing documentation](http://docs.scipy.org/doc/numpy/reference/routines.testing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_true_round(value):\n",
    "    # Default python3 round uses the round-to-even method (bankersâ€™ rounding)\n",
    "    # Here we use the round-half-away-from-zero method\n",
    "    rounded = (int(value + (0.5 if value > 0 else -0.5)))\n",
    "    return rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMyTrueRound(unittest.TestCase):\n",
    "\n",
    "    def test_positive(self):\n",
    "        self.assertEqual(my_true_round(1.3), 1)\n",
    "\n",
    "    def test_negative(self):\n",
    "        self.assertEqual(my_true_round(-1.3), -1)\n",
    "\n",
    "    def test_halfway_even(self):\n",
    "        self.assertEqual(my_true_round(2.5), 3)\n",
    "\n",
    "    def test_negative_halfway_even(self):\n",
    "        self.assertEqual(my_true_round(-2.5), -3)\n",
    "\n",
    "    def test_returned_type(self):\n",
    "        self.assertIsInstance(my_true_round(0.0), int)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest=\"TestMyTrueRound\")\n",
    "    # unittest.main(verbosity=2, defaultTest=\"TestMyTrueRound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Run from command line arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto discover tests of the current path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 -m unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a specific `TestCase`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 -m unittest myproject.test.TestMyTrueRound\n",
    "\n",
    "$ python3 test_builtin_round.py TestMyTrueRound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a specific test method:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 -m unittest myproject.test.TestMyTrueRound.test_positive\n",
    "\n",
    "$ python3 test_builtin_round.py TestMyTrueRound.test_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixture\n",
    "\n",
    "Tests might need to share some common initialisation/finalisation (e.g., create a temporary directory).\n",
    "\n",
    "This can be implemented in ``setUp`` and ``tearDown`` methods of ``TestCase``.\n",
    "Those methods are called before and after each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCaseWithFixture(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.file = open(\"img/test-pyramid.png\", \"rb\")\n",
    "        print(\"open file\")\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.file.close()\n",
    "        print(\"close file\")\n",
    "\n",
    "    def test1(self):\n",
    "        foo = self.file.read()\n",
    "        # do some processing on foo\n",
    "        print(\"processing1\")\n",
    "\n",
    "    def test2(self):\n",
    "        foo = self.file.read()\n",
    "        # do some processing on foo\n",
    "        print(\"processing2\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest='TestCaseWithFixture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyTrueRound(unittest.TestCase):\n",
    "\n",
    "    def test_bad_types(self):\n",
    "        try:\n",
    "            my_true_round('2')\n",
    "            self.fail()\n",
    "        except TypeError:\n",
    "            self.assertTrue(True)\n",
    "\n",
    "    def test_bad_types__better_way(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            my_true_round('2')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest='TestMyTrueRound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TestCase.assertRaisesRegexp` also checks the message of the exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric tests\n",
    "\n",
    "Running the same test with multiple values\n",
    "\n",
    "Problems:\n",
    "\n",
    "- The first failure stops the test, remaining test values are not processed.\n",
    "- There is no information on the value for which the test has failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyTrueRound(unittest.TestCase):\n",
    "\n",
    "    HALFWAY_TESTS = ((0.5, 1), (1.5, 2), (2.5, 3))\n",
    "\n",
    "    def test_all(self):\n",
    "        for value, expected in self.HALFWAY_TESTS:\n",
    "            self.assertEqual(my_true_round(value), expected)\n",
    "\n",
    "    def test_all__better_way(self):\n",
    "        for value, expected in self.HALFWAY_TESTS:\n",
    "            with self.subTest(value=value, expected=expected):\n",
    "                self.assertEqual(my_true_round(value), expected)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest='TestMyTrueRound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests\n",
    "\n",
    "How-to run tests from many ``TestCase`` and many files at once:\n",
    "\n",
    "- Explicit:\n",
    "    Full control, boilerplate code.\n",
    "\n",
    "- Automatic:\n",
    "    No control\n",
    "\n",
    "- Mixing approach\n",
    "\n",
    "\n",
    "The [TestSuite](https://docs.python.org/3/library/unittest.html#unittest.TestSuite) class aggregates test cases and test suites through:\n",
    "\n",
    "- Allow to test specific use cases\n",
    "- Full control of the test sequence\n",
    "- But requires some boilerplate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCase1(unittest.TestCase):\n",
    "    def test(self): pass\n",
    "class TestCase2(unittest.TestCase):\n",
    "    def test(self): pass\n",
    "class TestCaseGui1(unittest.TestCase):\n",
    "    def test(self): pass\n",
    "class TestCaseGui2(unittest.TestCase):\n",
    "    def test(self): pass\n",
    "\n",
    "def suite_without_gui():\n",
    "    loadTests = unittest.defaultTestLoader.loadTestsFromTestCase\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(loadTests(TestCase1))\n",
    "    suite.addTest(loadTests(TestCase2))\n",
    "    return suite\n",
    "\n",
    "def suite_with_gui():\n",
    "    loadTests = unittest.defaultTestLoader.loadTestsFromTestCase\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(suite_without_gui())\n",
    "    suite.addTest(loadTests(TestCaseGui1))\n",
    "    suite.addTest(loadTests(TestCaseGui2))\n",
    "    return suite\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # unittest.main(defaultTest='suite_without_gui')\n",
    "    unittest.main(defaultTest='suite_with_gui')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSample(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Called before all the tests of this class\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Called after all the tests of this class\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Module fixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpModule():\n",
    "    # Called before all the tests of this module\n",
    "    pass\n",
    "\n",
    "def tearDownModule():\n",
    "    # Called after all the tests of this module\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skipping tests\n",
    "\n",
    "If tests requires a specific OS, device, library..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest, socket\n",
    "\n",
    "def get_database_access():\n",
    "    socks = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    socks.settimeout(0.300)\n",
    "    try:\n",
    "        socks.connect((\"my_smart_database\", 42))\n",
    "    except:\n",
    "        return None\n",
    "    finally:\n",
    "        socks.close()\n",
    "\n",
    "@unittest.skipIf(get_database_access() is None, 'No database')\n",
    "class TestMyTrueRoundUsingDatabase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        if get_database_access() is None:\n",
    "            self.skipTest('No database')\n",
    "\n",
    "    def test1(self):\n",
    "        if get_database_access() is None:\n",
    "            self.skipTest('No database')\n",
    "        ...\n",
    "\n",
    "    @unittest.skipIf(get_database_access() is None, 'No database')\n",
    "    def test2(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest='TestMyTrueRoundUsingDatabase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test numpy\n",
    "\n",
    "Numpy provides modules for unittests. See the [Numpy testing documentation](http://docs.scipy.org/doc/numpy/reference/routines.testing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class TestNumpyArray(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.data1 = numpy.array([1, 2, 3, 4, 5, 6, 7])\n",
    "        self.data2 = numpy.array([1, 2, 3, 4, 5, 6, 7.00001])\n",
    "\n",
    "    # def test_equal__cant_work(self):\n",
    "    #     self.assertEqual(self.data1, self.data2)\n",
    "    #     self.assertTrue((self.data1 == self.data2).all())\n",
    "\n",
    "    def test_equal(self):\n",
    "        self.assertTrue(numpy.allclose(self.data1, self.data2, atol=0.0001))\n",
    "\n",
    "    def test_equal__even_better(self):\n",
    "        numpy.testing.assert_allclose(self.data1, self.data2, atol=0.0001)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest='TestNumpyArray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test resources\n",
    "\n",
    "How to handle test data?\n",
    "\n",
    "Need to separate (possibly huge) test data from python package.\n",
    "\n",
    "Download test data and store it in a temporary directory during the tests if not available.\n",
    "\n",
    "Example: [silx.utils.ExternalResources](https://github.com/silx-kit/silx/blob/master/silx/utils/utilstest.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# QTest\n",
    "\n",
    "For GUI based on `PyQt`, `PySide` it is possible to use Qt's [QTest](http://doc.qt.io/qt-5/qtest.html).\n",
    "\n",
    "It provides the basic functionalities for GUI testing.\n",
    "It allows to send keyboard and mouse events to widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from PyQt5 import Qt\n",
    "\n",
    "qapp = Qt.QApplication.instance()\n",
    "if Qt.QApplication.instance() is None:\n",
    "    qapp = Qt.QApplication([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from PyQt5 import Qt\n",
    "\n",
    "class MyApplication(Qt.QMainWindow):\n",
    "\n",
    "    def __init__(self, parent=None):\n",
    "        super(MyApplication, self).__init__(parent=parent)\n",
    "        self.initGui()\n",
    "\n",
    "    def initGui(self):\n",
    "        self._inputLine = Qt.QLineEdit(self)\n",
    "        self._processButton = Qt.QPushButton(self)\n",
    "        self._processButton.setText(\"Round\")\n",
    "        self._processButton.clicked.connect(self.processing)\n",
    "        self._resultWidget = Qt.QLabel(self)\n",
    "\n",
    "        widget = Qt.QWidget()\n",
    "        layout = Qt.QVBoxLayout(widget)\n",
    "        layout.addWidget(self._inputLine)\n",
    "        layout.addWidget(self._resultWidget)\n",
    "        layout.addWidget(self._processButton)\n",
    "        self.setCentralWidget(widget)\n",
    "\n",
    "    def processing(self):\n",
    "        text = self._inputLine.text()\n",
    "        data = float(text)\n",
    "        result = my_true_round(data)\n",
    "        self._resultWidget.setText(\"%d\" % result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtTest import QTest\n",
    "\n",
    "class TestGui(unittest.TestCase):\n",
    "\n",
    "    def test_type_and_process(self):\n",
    "        app = MyApplication()\n",
    "        QTest.qWaitForWindowExposed(app)\n",
    "        QTest.keyClicks(app._inputLine, '2.5', delay=100)  # Wait 100ms\n",
    "        QTest.mouseClick(app._processButton, Qt.Qt.LeftButton, pos=Qt.QPoint(1, 1))\n",
    "        self.assertEqual(app._resultWidget.text(), \"3\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(defaultTest='TestGui')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tighly coupled with the code it tests.\n",
    "It needs to know the widget's instance and hard coded position of mouse events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimate tests' quality\n",
    "\n",
    "Using [`coverage`](https://coverage.readthedocs.org) to gather coverage statistics while running the tests (`pip install coverage`)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python -m coverage run -m unittest\n",
    "$ python -m coverage report"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name                             Stmts   Miss  Cover\n",
    "----------------------------------------------------\n",
    "pypolynom\\__init__.py                1      0   100%\n",
    "pypolynom\\polynom.py                19      2    89%\n",
    "pypolynom\\test\\__init__.py           0      0   100%\n",
    "pypolynom\\test\\test_polynom.py      29      0   100%\n",
    "----------------------------------------------------\n",
    "TOTAL                               49      2    96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimate tests' quality\n",
    "\n",
    "Execute the tests and generate an output file per module with annotations per lines."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python -m coverage annotate\n",
    "\n",
    "$ ls pypolynom\n",
    "30/03/2019  19:15             1,196 polynom.py\n",
    "30/03/2019  19:17             1,294 polynom.py,cover"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> def polynom(a, b, c):\n",
    ">     delta = pow2(b) - 4.0 * a * c\n",
    ">     solutions = []\n",
    ">     if delta > 0:\n",
    "!         solutions.append((-b + sqrt(delta)) / (2.0 * a))\n",
    "!         solutions.append((-b - sqrt(delta)) / (2.0 * a))\n",
    ">     elif delta == 0:\n",
    ">         solutions.append(-b/(2.0*a))\n",
    ">     return solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration\n",
    "\n",
    "Automatically testing a software for each changes applied to the source code.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Be aware of problems early\n",
    "    - Before merging a change on the code\n",
    "    - On third-party library update (sometimes before the release)\n",
    "    - Reduce the cost in case of problem\n",
    "- Improve contributions and team work\n",
    "\n",
    "Costs:\n",
    "\n",
    "- Set-up and maintenance\n",
    "- Test needs to be automated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration\n",
    "\n",
    "- [Travis-CI](https://travis-ci.org/) (Linux and MacOS), [AppVeyor](http://www.appveyor.com/) (Windows), gitlab-CI (https://gitlab.esrf.fr)...\n",
    "- A `.yml` file to describing environment, build, installation, test process\n",
    "\n",
    "<img src=\"img/ci-workflow.svg\" style=\"width:40%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration: Configuration\n",
    "\n",
    "Example of configuration with Travis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "language: python\n",
    "\n",
    "matrix:\n",
    "    include:\n",
    "        - python: 3.6\n",
    "        - python: 3.7\n",
    "\n",
    "before_install: # Upgrade distribution modules\n",
    "    - python -m pip install --upgrade pip\n",
    "    - pip install --upgrade setuptools wheel\n",
    "\n",
    "install:        # Generate source archive and wheel\n",
    "    - python setup.py bdist_wheel\n",
    "\n",
    "before_script:  # Install wheel package\n",
    "    - pip install --pre dist/pypolynom*.whl\n",
    "\n",
    "script:         # Run the tests from the installed module\n",
    "    - mkdir tmp ; cd tmp\n",
    "    - python -m unittest discover pypolynom.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sum-up\n",
    "\n",
    "- Testing is a real job\n",
    "- The amount of work must not be under estimate. Designing good test is about 1/3 of resources a project\n",
    "- Tests should be done early, to identify design problems, and to improve team work\n",
    "- To be tested, an application have to be architectured in this way\n",
    "- Continuous integration is particularly useful to prevent regressions, and contributions\n",
    "- Aiming exhausting tests is not needed and utopic. A coverage of most cases is a very good start\n",
    "- Next step: Continuous deployment."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
