{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Testing\n",
    "=======\n",
    "\n",
    "- Introduction\n",
    "- Python `unittest` module\n",
    "- Additional tools\n",
    "- Continuous integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is it?\n",
    "\n",
    "- A task consisting of checking that the **program** is working as expected\n",
    "- Manually written **tests** which can be automatically executed\n",
    "- Different methodologies: Always and before anything else (test-driven development, *TDD*, [TDDwithPython])\n",
    "\n",
    "<img src=\"img/ttd-workflow.png\" style=\"width:40%;margin-left:auto;margin-right:auto;padding:2em;\">\n",
    "\n",
    "[TDDwithPython] `Harry J.W. Percival. Test-Driven Development with Python. O'Reilly 2014. [Oriented towards web development](http://chimera.labs.oreilly.com/books/1234000000754)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "- Test inject input to the program, and check output\n",
    "- Answer valid or not\n",
    "- For maintenance: Reproduce bug in a test, then fix it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why testing?\n",
    "\n",
    "- Validate the code to the specifications\n",
    "- Find problems early\n",
    "- Facilitates change\n",
    "- Documentation\n",
    "- Code quality\n",
    "\n",
    "  - Better design\n",
    "  - Simplifies integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why not?\n",
    "\n",
    "- Extra work\n",
    "- Not perfect\n",
    "- Extra maintenance\n",
    "\n",
    "  - More difficult to refactoring\n",
    "  - Maintain environments\n",
    "\n",
    "- Delays integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "- 30% percent of the time of the project\n",
    "- Having the structure set-up for testing encourages writing tests.\n",
    "- Then... let's talk about the structure :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What kinds of tests?\n",
    "\n",
    "- **Functional tests**: Tests scripts, public API, GUI\n",
    "- **Integration tests**: Tests components, code correctly integrated\n",
    "- **Unit tests**: Tests independant pieces of code\n",
    "\n",
    "<img src=\"img/test-pyramid.png\" style=\"width:40%;margin-left:auto;margin-right:auto;padding:0em;\">\n",
    "\n",
    "[TestPyramid] Mike Cohn. Succeeding with Agile: Software Development Using Scrum. 2009.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "The test pyramid is a concept developed by Mike Cohn, described in his book \"Succeeding with Agile\"\n",
    "\n",
    "- Unit tests (dev point of view, fast, low cost)\n",
    "- Integration tests\n",
    "- Functional tests (user point of view, but slow, and expensive)\n",
    "\n",
    "- Cost: unit << integration << functional\n",
    "- Fast to execute: unit >> integration >> functional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where to put the tests?\n",
    "\n",
    "Separate tests from the source code:\n",
    "\n",
    "- Run the test from the command line.\n",
    "- Separate test code when distributing.\n",
    "- [...](https://docs.python.org/3/library/unittest.html#organizing-test-code)\n",
    "\n",
    "Folder structure:\n",
    "\n",
    "- In a separate `test/` folder.\n",
    "- In `test` sub-packages in each Python package/sub-package,\n",
    "  so that tests remain close to the source code.\n",
    "  Tests are installed with the package and can be run from the installation.\n",
    "- A `test_*.py` for each module and script (an more if needed).\n",
    "- Consider separating tests that are long to run from the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where to put the tests?\n",
    "\n",
    "- project\n",
    "  - setup.py\n",
    "  - run_tests.py\n",
    "  - package/\n",
    "    - __init__.py\n",
    "    - module1.py\n",
    "    - test/\n",
    "      - __init__.py\n",
    "      - test_module1.py\n",
    "    - subpackage/\n",
    "      - __init__.py\n",
    "      - module1.py\n",
    "      - module2.py\n",
    "      - test/\n",
    "        - __init__.py\n",
    "        - test_module1.py\n",
    "        - test_module2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Presenter Notes\n",
    "\n",
    "- scripts/\n",
    "  - my_script.py\n",
    "  - my_other_script.py\n",
    "- test/\n",
    "  - test_my_script.py\n",
    "  - test_my_other_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest` Python module\n",
    "\n",
    "- QuickStart\n",
    "- Chaining tests\n",
    "- Running tests\n",
    "- More on TestCase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# QuickStart\n",
    "\n",
    "[unittest](https://docs.python.org/3/library/unittest.html) is the default Python module for testing.\n",
    "\n",
    "It provides features to:\n",
    "\n",
    "- Write tests\n",
    "- Discover tests\n",
    "- Run those tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TestCase\n",
    "\n",
    "The classe `unittest.TestCase` is the base class for writting tests for\n",
    "Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMyTestCase(unittest.TestCase):\n",
    "\n",
    "    def test_my_test(self):\n",
    "        # Test code\n",
    "        self.assertEqual(a, b, message=None)\n",
    "\n",
    "    def test_another_test(self):\n",
    "        self.assertTrue(x, message=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assertion functions\n",
    "\n",
    "- Argument(s) to compare/evaluate.\n",
    "- An additional error message.\n",
    "\n",
    "- `assertEqual(a, b)` checks that `a == b`\n",
    "- `assertNotEqual(a, b)` checks that `a != b`\n",
    "- `assertTrue(x)` checks that `bool(x) is True`\n",
    "- `assertFalse(x)`checks that `bool(x) is False`\n",
    "- `assertIs(a, b)` checks that `a is b`\n",
    "- `assertIsNone(x)` checks that `x is None`\n",
    "- `assertIn(a, b)` checks that `a in b`\n",
    "- `assertIsInstance(a, b)` checks that `isinstance(a, b)`\n",
    "\n",
    "There's more, see [unittest TestCase documentation](https://docs.python.org/3/library/unittest.html#unittest.TestCase>)\n",
    "or [Numpy testing documentation](http://docs.scipy.org/doc/numpy/reference/routines.testing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The function `unittest.main()` provides a command line interface to\n",
    "discover and run the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestBuiltInRound(unittest.TestCase):\n",
    "\n",
    "    def test_positive(self):\n",
    "        result = round(1.3)\n",
    "        self.assertEqual(result, 1)\n",
    "\n",
    "    def test_negative(self):\n",
    "        result = round(-1.3)\n",
    "        self.assertEqual(result, -1)\n",
    "\n",
    "    def test_halfway_even(self):\n",
    "        result = round(2.5)\n",
    "        self.assertEqual(result, 2, msg=\"round(2.5) -> %f != 2\" % result)\n",
    "\n",
    "    def test_returned_type(self):\n",
    "        self.assertIsInstance(round(0.), int)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Result in Python3\n",
    "\n",
    "Running tests from the command line on Python3:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 test_builtin_round.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  ....\n",
    "  ----------------------------------------------------------------------\n",
    "  Ran 4 tests in 0.000s\n",
    "\n",
    "  OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Make it fail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python2 test_builtin_round.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "F..F\n",
    "====\n",
    "FAIL: test_halfway_even (__main__.TestRound)\n",
    "--------------------------------------------\n",
    "Traceback (most recent call last):\n",
    " File \"test_builtin_round.py\", line 16, in test_halfway_even\n",
    "   self.assertEqual(result, 2, msg=\"round(2.5) -> %f != 2\" % result)\n",
    "AssertionError: round(2.5) -> 3.000000 != 2\n",
    "\n",
    "======================================================================\n",
    "FAIL: test_returned_type (__main__.TestRound)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    " File \"test_builtin_round.py\", line 19, in test_returned_type\n",
    "   self.assertIsInstance(round(0.), int)\n",
    "AssertionError: 0.0 is not an instance of <type 'int'>\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 4 tests in 0.000s\n",
    "\n",
    "FAILED (failures=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Command line arguments\n",
    "\n",
    "Running a specific `TestCase`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 test_builtin_round.py TestBuiltInRound"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "....\n",
    "-------------------------------------------------------------\n",
    "Ran 4 tests in 0.000s\n",
    "\n",
    "OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a specific test method:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 test_builtin_round.py TestBuiltInRound.test_positive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".\n",
    "-------------------------------------------------------------\n",
    "Ran 1 test in 0.000s\n",
    "\n",
    "OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests\n",
    "\n",
    "How-to run tests from many ``TestCase`` and many files at once:\n",
    "\n",
    "- Explicit:\n",
    "    Full control, boilerplate code.\n",
    "\n",
    "- Automatic:\n",
    "    No control\n",
    "\n",
    "- Mixing approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests: Suite\n",
    "\n",
    "The [TestSuite](https://docs.python.org/3/library/unittest.html#unittest.TestSuite) class aggregates test cases and test suites through:\n",
    "\n",
    "- `TestSuite.addTest(test)`\n",
    "- `TestSuite.addTests(test)`\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = unittest.TestSuite()\n",
    "suite.addTest(TestBuiltInRound('test_positive'))\n",
    "...\n",
    "...\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests: Loader\n",
    "\n",
    "`unittest.defaultTestLoader` (an instance of `unittest.TestLoader`) creates `TestSuite` from classes and modules.\n",
    "\n",
    "`TestLoader.loadTestsFromTestCase(testCaseClass)`` method creates a `TestSuite` from all `test*` method of a `TestCase` subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadTests = unittest.defaultTestLoader.loadTestsFromTestCase\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(loadTests(TestBuiltInRound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests: Module\n",
    "\n",
    "First, write a ``suite`` function for each module (i.e., file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suite():\n",
    "    loadTests = unittest.defaultTestLoader.loadTestsFromTestCase\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(loadTests(TestBuiltInRound))\n",
    "    return suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests: Package\n",
    "\n",
    "Then a ``suite`` function collecting all tests in a package (i.e., directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from . import test_builtin_round\n",
    "...\n",
    "\n",
    "def suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(test_builtin_round.suite())\n",
    "    ...\n",
    "    return suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to create a `TestSuite` from all tests in a project:\n",
    "\n",
    "- Full control over the creation of the `TestSuite`.\n",
    "- Requires some boilerplate code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chaining tests: Runner\n",
    "\n",
    "To run the ``suite`` from command line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suite():\n",
    "    ...\n",
    "\n",
    "if __name__ == \"__main__\":  # True if run as a script\n",
    "    unittest.main(defaultTest='suite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project: Running tests\n",
    "\n",
    "- `unittest.main` to run each module independantly.\n",
    "- Command line: `python -m unittest ...`\n",
    "- With a `run_tests.py` script.\n",
    "\n",
    "Minimal run_tests.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import mymodule.tests\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(mymodule.tests.suite())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sum-up\n",
    "\n",
    "- For each modules\n",
    "    - Write a test module with tests as `TestCase` sub-class\n",
    "    - Use `assert` methods in the tests\n",
    "    - Run the tests as a script from the command line\n",
    "\n",
    "- For packages and project\n",
    "    - Chain tests with `TestSuite`\n",
    "    - Create a script to run your tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More features\n",
    "\n",
    "- Fixture\n",
    "- Testing exception\n",
    "- Skipping tests\n",
    "- Parametric tests\n",
    "- Test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixture\n",
    "\n",
    "Tests might need to share some common initialisation/finalisation (e.g., create a temporary directory).\n",
    "\n",
    "This can be implemented in ``setUp`` and ``tearDown`` methods of ``TestCase``.\n",
    "Those methods are called before and after each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCaseWithFixture(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        pass  # Pre-test code\n",
    "\n",
    "    def tearDown(self):\n",
    "        pass  # Post-test code\n",
    "\n",
    "    pass  # Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Module fixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpModule():\n",
    "    # Called before all the tests of this module\n",
    "    pass\n",
    "\n",
    "def tearDownModule():\n",
    "    # Called after all the tests of this module\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSample(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Called before all the tests of this class\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Called after all the tests of this class\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBuiltInRound(unittest.TestCase):\n",
    "\n",
    "    def test_raise_type_error(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            result = round('2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TestCase.assertRaisesRegexp` also checks the message of the exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skipping tests\n",
    "\n",
    "Why skipping a test: Test requires a specific OS or a specific version of a library...\n",
    "\n",
    "To skip a test, call `TestCase.skipTest(reason)` from the test* or `setUp` method.\n",
    "\n",
    "Also available through decorators `unittest.skip`, `unittest.skipIf`, `unittest.skipUnless`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unittest\n",
    "\n",
    "class TestBuiltInRound(unittest.TestCase):\n",
    "\n",
    "    def test_python2(self):\n",
    "        if sys.version_info[0] != 2:\n",
    "            self.skipTest('Requires Python 2')\n",
    "        self.assertEqual(round(2.5), 3.0)\n",
    "\n",
    "    @unittest.skipIf(sys.version_info[0] != 3, 'Requires Python 3')\n",
    "    def test_python3(self):\n",
    "        self.assertEqual(round(2.5), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric tests\n",
    "\n",
    "Running the same test with multiple values\n",
    "\n",
    "Problems:\n",
    "\n",
    "- The first failure stops the test, remaining test values are not processed.\n",
    "- There is no information on the value for which the test has failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBuiltInRound(unittest.TestCase):\n",
    "\n",
    "    HALFWAY_TESTS = ((0.5, 0), (1.5, 2), (2.5, 2))\n",
    "\n",
    "    def test_all(self):\n",
    "        for value, expected in self.HALFWAY_TESTS:\n",
    "            self.assertEqual(round(value), expected)\n",
    "\n",
    "    def test_all_better_way(self):\n",
    "        for value, expected in self.HALFWAY_TESTS:\n",
    "            with self.subTest(value=value, expected=expected):\n",
    "                self.assertEqual(round(value), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test data\n",
    "\n",
    "How to handle test data?\n",
    "\n",
    "Need to separate (possibly huge) test data from python package.\n",
    "\n",
    "Download test data and store it in a temporary directory during the tests if not available.\n",
    "\n",
    "Example: [silx.utils.ExternalResources](https://github.com/silx-kit/silx/blob/master/silx/utils/utilstest.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# QTest\n",
    "\n",
    "For GUI based on `PyQt`, `PySide` it is possible to use Qt's [QTest](http://doc.qt.io/qt-5/qtest.html).\n",
    "\n",
    "It provides the basic functionalities for GUI testing.\n",
    "It allows to send keyboard and mouse events to widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from PyQt5 import Qt\n",
    "\n",
    "qapp = Qt.QApplication.instance()\n",
    "if Qt.QApplication.instance() is None:\n",
    "    qapp = Qt.QApplication([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from PyQt5 import Qt\n",
    "\n",
    "class MyApplication(Qt.QMainWindow):\n",
    "\n",
    "    def __init__(self, parent=None):\n",
    "        super(MyApplication, self).__init__(parent=parent)\n",
    "        self.initGui()\n",
    "\n",
    "    def initGui(self):\n",
    "        self._inputLine = Qt.QLineEdit(self)\n",
    "        self._processButton = Qt.QPushButton(self)\n",
    "        self._processButton.setText(\"Process\")\n",
    "        self._processButton.clicked.connect(self.processing)\n",
    "        self._resultWidget = Qt.QLabel(self)\n",
    "\n",
    "        widget = Qt.QWidget()\n",
    "        layout = Qt.QVBoxLayout(widget)\n",
    "        layout.addWidget(self._inputLine)\n",
    "        layout.addWidget(self._resultWidget)\n",
    "        layout.addWidget(self._processButton)\n",
    "        self.setCentralWidget(widget)\n",
    "\n",
    "    def processing(self):\n",
    "        # Compute the sum of values displayed in the line edit\n",
    "        text = self._inputLine.text()\n",
    "        data = [int(d) for d in text.split()]\n",
    "        self.result = sum(data)\n",
    "        self._resultWidget.setText(\"%d\" % self.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import Qt\n",
    "from PyQt5.QtTest import QTest\n",
    "\n",
    "app = MyApplication()\n",
    "app.show()\n",
    "QTest.qWaitForWindowExposed(widget)\n",
    "QTest.keyClicks(app._inputLine, '1 10 100', delay=100)  # Wait 100ms\n",
    "QTest.mouseClick(app._processButton, QtCore.Qt.LeftButton, pos=QtCore.QPoint(1, 1))\n",
    "assert app._resultWidget.text() == \"111\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tighly coupled with the code it tests.\n",
    "It needs to know the widget's instance and hard coded position of mouse events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test coverage\n",
    "\n",
    "Using [`coverage`](https://coverage.readthedocs.org) to gather coverage statistics while running the tests:\n",
    "\n",
    "- Install `coverage.py` package: `pip install coverage`.\n",
    "- Run the tests: `python -m coverage run --source <package_dir> run_tests.py`\n",
    "- Show report:\n",
    "\n",
    "  - `python -m coverage report`\n",
    "  - `python -m coverage html`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Name                                   Stmts   Miss  Cover\n",
    "  ----------------------------------------------------------\n",
    "  rounding/__init__                          5      1    80%\n",
    "  rounding/tests/__init__                   13      4    69%\n",
    "  rounding/tests/test_parametric_round      27      1    96%\n",
    "  rounding/tests/test_round                 23      1    96%\n",
    "  ----------------------------------------------------------\n",
    "  TOTAL                                     68      7    90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extra test tools\n",
    "\n",
    "Extending `unittest`:\n",
    "\n",
    "- [pytest](http://pytest.org/)\n",
    "- [nose](https://nose.readthedocs.org/)\n",
    "\n",
    "Running the tests on different Python environments:\n",
    "\n",
    "- [tox](https://tox.readthedocs.org/) - automates testing of Python packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Test on multiple platform/configuration (e.g., different version of Python).\n",
    "- Test often: each commit, each pull request, daily...\n",
    "\n",
    "Costs:\n",
    "\n",
    "- Set-up and maintenance.\n",
    "- Test needs to be automated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration\n",
    "\n",
    "- [Travis-CI](https://travis-ci.org/): Linux and MacOS\n",
    "- [AppVeyor](http://www.appveyor.com/): Windows\n",
    "- gitlab-CI (https://gitlab.esrf.fr)\n",
    "- ...\n",
    "\n",
    "Principle:\n",
    "\n",
    "- Add a `.yml` file to your repository describing:\n",
    "\n",
    "  - The test environment\n",
    "  - Build and installation of the dependencies and the package\n",
    "  - The way to run the tests.\n",
    "\n",
    "- Upon commit, clones the repository and runs the tests.\n",
    "- Displays the outcome on a web page.\n",
    "- Feedback github Pull Requests with test status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continuous integration: Configuration\n",
    "\n",
    "Example of configuration with Travis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "language: python\n",
    "\n",
    "matrix:\n",
    "    include:\n",
    "        - python: 3.6\n",
    "        - python: 3.7\n",
    "\n",
    "before_install: # Upgrade distribution modules\n",
    "    - python -m pip install --upgrade pip\n",
    "    - pip install --upgrade setuptools wheel\n",
    "\n",
    "install:        # Generate source archive and wheel\n",
    "    - python setup.py bdist_wheel\n",
    "\n",
    "before_script:  # Install wheel package\n",
    "    - pip install --pre dist/pypolynom*.whl\n",
    "\n",
    "script:         # Run the tests from the installed module\n",
    "    - mkdir tmp ; cd tmp\n",
    "    - python -m unittest discover pypolynom.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sum-up\n",
    "\n",
    "- Different test strategies.\n",
    "- Python `unittest` (and extra packages) to write and run the tests.\n",
    "- Additional tools to efficiently run the tests: Continuous Integration.\n",
    "- Next step: Continuous Deployment."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
