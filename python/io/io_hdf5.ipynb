{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data IO (input/output)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data comes in (too many) different formats:\n",
    "\n",
    "* Specfile\n",
    "* EDF\n",
    "* HDF5\n",
    "\n",
    "And specific detector formats:\n",
    "\n",
    "* MarCCD\n",
    "* Pilatus CBF\n",
    "* Dectris Eiger\n",
    "* â€¦\n",
    "\n",
    "\n",
    "HDF5 is now the standard ESRF data format so we will only focus on it today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5\n",
    "\n",
    "![hdf_group](images/HDF_logo.png \"HDF group\")\n",
    "\n",
    "## what is hdf5 ?\n",
    "\n",
    "[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) (for Hierarchical Data Format) is a file format to structure and store data for high volume and complex data\n",
    "\n",
    "## Why hdf5 ?\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Portable file format (Standard exchange format for heterogeneous data)\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "* Free ( & open source)\n",
    "* Adopted by a large number of institute (NASA, LIGO, ...)\n",
    "* Adopted by most of the synchrotrons (esrf, SOLEIL, Daisy...)\n",
    "* Insure [forward and backward compatibility](https://support.hdfgroup.org/HDF5/doc/ADGuide/CompatFormat180.html)\n",
    "\n",
    "**Data can be mostly anything: image, table, graphs, documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes)\n",
    "\n",
    "![hdf5_class_diag](images/hdf5_model.png \"hdf5 class diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by [pyFAI](https://github.com/silx-kit/pyFAI)\n",
    "\n",
    "![hdf5_example](images/hdf5_example.png \"hdf5 example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull tools for HDF5\n",
    "\n",
    "* `h5ls`, `h5dump`, `hdfview`\n",
    "```bash\n",
    ">>> h5ls -r my_first_one.h5 \n",
    ">>> /                        Group\n",
    ">>> /data1                   Dataset {100, 100}\n",
    ">>> /group1                  Group\n",
    ">>> /group1/data2            Dataset {100, 100}\n",
    "```\n",
    "\n",
    "* `silx view`\n",
    "\n",
    "```bash\n",
    ">>> pip install silx\n",
    ">>> silx view my_file.h5\n",
    "```\n",
    "\n",
    "* `h5glance`: File browser for jupyter\n",
    "\n",
    "==> The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h5py\n",
    "\n",
    "![h5py book](images/h5py.gif \"h5py book\")\n",
    "\n",
    "[h5py](https://www.h5py.org/) is the python binding for accessing hdf5. Originally from [Andrew Collette](http://shop.oreilly.com/product/0636920030249.do)\n",
    "\n",
    "With time work more and more closely with the hdfgroup.\n",
    "\n",
    "Easy to associate hdf5 and python, everything is represented as a dictionnary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read an hdf5 file with h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first open a file using a [File Object](http://docs.h5py.org/en/stable/high/file.html)\n",
    "```\n",
    "h5py.File('myfile.hdf5', opening_mode)\n",
    "```\n",
    "\n",
    "[opening modes](http://docs.h5py.org/en/stable/high/file.html#opening-creating-files) are:\n",
    "\n",
    "|         |                                                  |\n",
    "|---------|--------------------------------------------------|\n",
    "| r       | Readonly, file must exist                        |\n",
    "| r+      | Read/write, file must exist                      |\n",
    "| w       | Create file, truncate if exists                  |\n",
    "| w- or x | Create file, fail if exists                      |\n",
    "| a       | Read/write if exists, create otherwise (default) |\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessing ESRF data\n",
    "\n",
    "## Library\n",
    "\n",
    "\n",
    "Those are already available for most ESRF computers."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apt-get install python3-h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross platform (available for Windows, Linux, Mac OS X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also available from source code (under MIT license)\n",
    "\n",
    "* https://github.com/h5py/h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", h5file['/'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# Here we only read metadata from the dataset\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to close the file\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or better, use a context manager\n",
    "# The file is closed for you\n",
    "with h5py.File('data/test.h5', \"r\") as h5file:\n",
    "    print(h5file['/'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDF5 mimics numpy-array\n",
    "\n",
    "The data is read from the file only when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "dataset = h5file['/diff_map_0004/data/map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read and apply an operation\n",
    "print(dataset[5, 5, 0:5])\n",
    "print(2 * dataset[0, 5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data and store it as a numpy-array\n",
    "b = dataset[...]\n",
    "b[0, 0, 0:5] = 0\n",
    "print(dataset[0, 0, 0:5])\n",
    "print(b[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Write a HDF5 file\n",
    "\n",
    "* http://docs.h5py.org/en/stable/high/group.html\n",
    "* http://docs.h5py.org/en/stable/high/dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create a 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# Notice the mode='w', as 'write'\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file['/data1'] = data\n",
    "\n",
    "    # write data into a dataset from group1\n",
    "    h5file['/group1/data2'] = data\n",
    "\n",
    "    # Or with a functional API\n",
    "    g = h5file.create_group(\"/group2\")\n",
    "    g.create_dataset(\"data3\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercice: Flat field correction\n",
    "\n",
    "Flat-field correction is a technique used to improve quality in digital imaging.\n",
    "\n",
    "The goal is to normalize images and remove artifacts caused by variations in the pixel-to-pixel sensitivity of the detector and/or by distortions in the optical path. (see https://en.wikipedia.org/wiki/Flat-field_correction)\n",
    "\n",
    "$$ normalized = \\frac{raw - dark}{flat - dark} $$\n",
    "\n",
    "* `normalized`: Image after flat field correction\n",
    "* `raw`: Raw image. It is acquired with the sample.\n",
    "* `flat`: Flat field image. It is the response given out by the detector for a uniform input signal. This image is acquired without the sample.\n",
    "* `dark`: Also named `background` or `dark current`. It is the response given out by the detector when there is no signal. This image is acquired without the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. Browse the file ``data/ID16B_diatomee.h5``\n",
    "2. Reach a single raw data, a flat field and a dark image from this file\n",
    "3. Apply the flat field correction\n",
    "4. Save the result into a new HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise1.py](./solutions/exercise1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/ID16B_diatomee.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Read the data\n",
    "\n",
    "...\n",
    "\n",
    "# Compute the result\n",
    "\n",
    "normalized = flatfield_correction(raw, flat, dark)\n",
    "\n",
    "# Save the result\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "1. Apply the flat field correction to all raw data available (use the same flat and dark for all the images)\n",
    "2. Save each result into different datasets of the same HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise2.py](./solutions/exercise2.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "From the previous exercise, we can see that the flat field correction was not very good for the last images.\n",
    "\n",
    "Another flat field was acquired at the end of the acquisition.\n",
    "\n",
    "We could use this information to compute a flat field closer to the image we want to normalize. It can be done with a linear interpolation of the flat images by using the name of the image as the interpolation factor (which varies between 0 and 500 in this case).\n",
    "\n",
    "1. For each raw data, compute the corresponding flat field using lineal interpolation (between `flatfield/0000` and `flatfield/0500`)\n",
    "2. Save each result into different datasets in a single HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise3.py](./solutions/exercise3.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Preconized libraries according to the use case and the file format.\n",
    "\n",
    "| Formats              | Read            | Write |\n",
    "|----------------------|-----------------|-------|\n",
    "| HDF5                 | silx/h5py       | h5py  |\n",
    "| Specfile             | silx            |       |\n",
    "| EDF                  | silx/fabio      | fabio |\n",
    "| Other raster formats | silx/fabio      | fabio |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Utils: Conversion tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- `fabio-convert`: To convert raster images \n",
    "- `silx convert`: To convert EDF, or spec files to HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nexus](https://www.nexusformat.org/) is a data format for neutron, x-ray, and muon science.\n",
    "\n",
    "It defined a common to represente dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
