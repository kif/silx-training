{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data IO (input/output)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data (used to) come in (too many) different formats:\n",
    "\n",
    "* Specfile, EDF, HDF5\n",
    "* And specific detector formats: MarCCD, Pilatus CBF, Dectris Eiger, â€¦\n",
    "\n",
    "\n",
    "HDF5 is now the standard ESRF data format so we will only focus on it today.\n",
    "\n",
    "Methods for accessing other file formats are described in the [io_spec_edf.ipynb](io_spec_edf.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5\n",
    "\n",
    "## What is HDF5 ?\n",
    "\n",
    "[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) (for Hierarchical Data Format) is a file format to structure and store complex and large volumes of data.\n",
    "\n",
    "## Why HDF5 ?\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Portable file format (Standard exchange format for heterogeneous data)\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "* Free ( & open source)\n",
    "* Adopted by a large number of institutes (NASA, LIGO, ...)\n",
    "* Adopted by most of the synchrotrons (ESRF, SOLEIL, Desy...)\n",
    "\n",
    "**Data can be mostly anything: image, table, graphs, documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes, virtual datasets)\n",
    "\n",
    "![hdf5_class_diag](images/hdf5_model.png \"hdf5 class diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by [pyFAI](https://github.com/silx-kit/pyFAI)\n",
    "\n",
    "![hdf5_example](images/hdf5_example.png \"hdf5 example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Useful tools for HDF5\n",
    "\n",
    "* `h5ls`, `h5dump`, `hdfview`\n",
    "```bash\n",
    ">>> h5ls -r my_first_one.h5\n",
    "    /                        Group\n",
    "    /data1                   Dataset {100, 100}\n",
    "    /group1                  Group\n",
    "    /group1/data2            Dataset {100, 100}\n",
    "```\n",
    "\n",
    "* `silx view`\n",
    "\n",
    "```bash\n",
    ">>> pip install silx\n",
    ">>> silx view my_file.h5\n",
    "```\n",
    "\n",
    "* `h5glance`: File browser for jupyter\n",
    "\n",
    "* `h5py`: Access HDF5 files from python\n",
    "\n",
    "==> The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# h5py\n",
    "\n",
    "![h5py book](images/h5py.png \"h5py book\")\n",
    "\n",
    "[h5py](https://www.h5py.org/) is the python binding for accessing HDF5 files. Originally from [Andrew Collette](http://shop.oreilly.com/product/0636920030249.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Warning**: A major version of h5py has been released at the end of 2020 (3.0).\n",
    "Some code might not be compatible with it yet (end of python2 support, [interface for storing & reading strings has changed](https://docs.h5py.org/en/stable/strings.html)...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "print('h5py:', h5py.version.version)\n",
    "print('hdf5:', h5py.version.hdf5_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to read a HDF5 file\n",
    "\n",
    "* First open the file with [h5py.File](http://docs.h5py.org/en/stable/high/file.html):\n",
    "  ```\n",
    "  h5py.File('myfile.hdf5', mode)\n",
    "  ```\n",
    "  [opening modes](http://docs.h5py.org/en/stable/high/file.html#opening-creating-files):\n",
    "\n",
    "| Mode    | Meaning                                                            |\n",
    "|---------|--------------------------------------------------------------------|\n",
    "| r       | Readonly, file must exist; *Default with h5py* **v3**              |\n",
    "| r+      | Read/write, file must exist                                        |\n",
    "| w       | Create file, truncate if exists                                    |\n",
    "| w- or x | Create file, fail if exists                                        |\n",
    "| a       | Read/write if exists, create otherwise; *Default with h5py* **v2** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Then access the file content with a dictionary-like API, [h5py.Group](http://docs.h5py.org/en/stable/high/group.html):\n",
    "\n",
    "  - [`Group.keys()`](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.keys)\n",
    "  - [`Group.items()`](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.items)\n",
    "  - [`Group.values()`](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Available names at the first level\n",
    "h5file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 'diff_map_0000' group children\n",
    "group = h5file['diff_map_0004']\n",
    "dict(group.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 'diff_map_0000/data' group children\n",
    "list(h5file[\"/diff_map_0004/data\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get a dataset from a sub group\n",
    "h5dataset = h5file['/diff_map_0004/data/map']\n",
    "h5dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not very convenient for interactive browsing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to browse a HDF5 file: h5glance and silx view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# From jupyter notebook\n",
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# From the command line\n",
    "h5glance data/test.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# With silx view GUI\n",
    "silx view data/test.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### h5py.Dataset\n",
    "\n",
    "It mimics `numpy.ndarray`.\n",
    "The data is read from the file only when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "h5dataset = h5file['/diff_map_0004/data/map']\n",
    "h5dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here we only read metadata from the dataset\n",
    "print(\"Dataset:\", h5dataset.shape, h5dataset.dtype, h5dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from the file to a numpy.ndarray\n",
    "subset = h5dataset[5, 5, 0:5]  # Copy the selection to a numpy.ndarray\n",
    "print('subset:', subset, '=> sum:', subset.sum())\n",
    "\n",
    "data = h5dataset[()]  # Copy the whole dataset to a numpy.ndarray\n",
    "print('data type:', type(data), '; shape', data.shape, '; min.:', data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data[0, 0, 0:5] = 0\n",
    "print(data[0, 0, 0:5])\n",
    "print(h5dataset[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Remember to close the file\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(data[0, 0, 0:5])\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Once the file is closed, the Dataset no longer gives access to data\n",
    "print(h5dataset)\n",
    "print(h5dataset[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Context manager\n",
    "\n",
    "* Context managers guarantee that resources are released. In our case, it ensures that the HDF5 file is closed.\n",
    "* Usually used from the `with` statement.\n",
    "\n",
    "To safely access a HDF5 file, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File('data/test.h5', \"r\") as h5file:\n",
    "    dataset = h5file['/diff_map_0004/data/map']\n",
    "    data = dataset[()]\n",
    "print(dataset)\n",
    "print(data[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to write in a HDF5 file\n",
    "\n",
    "*There are several ways for writing groups and datasets. Here we will mostly focus on the 'dictionary' like API.*\n",
    "\n",
    "Documentation: [Group](http://docs.h5py.org/en/stable/high/group.html) and [Dataset](http://docs.h5py.org/en/stable/high/dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "data = numpy.random.random(10000)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# create file, notice the mode='w', as 'write'\n",
    "h5file = h5py.File('my_first_one.h5', mode='w')\n",
    "\n",
    "# write data into a dataset from the root\n",
    "h5file['/data1'] = data\n",
    "\n",
    "# write data into a dataset from group1\n",
    "h5file['/group1/data2'] = data\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same operation with a context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file['/data1'] = data\n",
    "\n",
    "    # write data into a dataset from group1\n",
    "    h5file['/group1/data2'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With [Group.create_dataset](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset) and [Group.create_group](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_group):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# Notice the mode='w', as 'write'\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file.create_dataset('data1', data=data)\n",
    "\n",
    "    # Or with a functional API\n",
    "    grp1 = h5file.create_group(\"group1\")\n",
    "    grp1.create_dataset(\"data2\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercice: Flat field correction\n",
    "\n",
    "Flat-field correction is a technique used to improve quality in digital imaging.\n",
    "\n",
    "The goal is to normalize images and remove artifacts caused by variations in the pixel-to-pixel sensitivity of the detector and/or by distortions in the optical path. (see https://en.wikipedia.org/wiki/Flat-field_correction)\n",
    "\n",
    "$$ normalized = \\frac{raw - dark}{flat - dark} $$\n",
    "\n",
    "* `normalized`: Image after flat field correction\n",
    "* `raw`: Raw image. It is acquired with the sample.\n",
    "* `flat`: Flat field image. It is the response given out by the detector for a uniform input signal. This image is acquired without the sample.\n",
    "* `dark`: Also named `background` or `dark current`. It is the response given out by the detector when there is no signal. This image is acquired without the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is a function implementing the flat field correction:\n",
    "\n",
    "*Note: make sure you execute the cell for defining this function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def flatfield_correction(raw, flat, dark):\n",
    "    \"\"\"\n",
    "    Apply a flat-field correction to a raw data using a flat and a dark.\n",
    "    \"\"\"\n",
    "    # Make sure that the computation is done using float\n",
    "    # to avoid type overflow or loss of precision\n",
    "    raw = raw.astype(numpy.float32)\n",
    "    flat = flat.astype(numpy.float32)\n",
    "    dark = dark.astype(numpy.float32)\n",
    "    # Do the computation\n",
    "    return (raw - dark) / (flat - dark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: If you like to plot an image you can use `matplotlib`'s `imshow` function.\n",
    "\n",
    "The `%matplotlib` \"magic\" command should be called once first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "plt.imshow(numpy.random.random((20, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Browse the file ``data/ID16B_diatomee.h5``\n",
    "2. Get a single raw dataset, a flat field dataset and a dark image dataset from this file\n",
    "3. Apply the flat field correction\n",
    "4. Save the result into a new HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise1.py](./solutions/exercise1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/ID16B_diatomee.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"data/ID16B_diatomee.h5\", mode=\"r\") as h5s:\n",
    "    pass\n",
    "    # this is a comment\n",
    "\n",
    "    # step1: Read the data\n",
    "\n",
    "    # raw_data_path = ...\n",
    "    # raw_data = ...\n",
    "\n",
    "    # flat_path = ...\n",
    "    # flat = ...\n",
    "\n",
    "    # dark_path = ...\n",
    "    # dark = ...\n",
    "\n",
    "# step2: Compute the result\n",
    "\n",
    "# normalized = flatfield_correction(raw_data, flat, dark)\n",
    "\n",
    "# step3: Save the result\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Apply the flat field correction to all raw data available (use the same flat and dark for all the images)\n",
    "2. Save each result into different datasets of the same HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise2.py](./solutions/exercise2.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "From the previous exercise, we can see that the flat field correction was not very good for the last images.\n",
    "\n",
    "Another flat field was acquired at the end of the acquisition.\n",
    "\n",
    "We could use this information to compute a flat field closer to the image we want to normalize. It can be done with a linear interpolation of the flat images by using the name of the image as the interpolation factor (which varies between 0 and 500 in this case).\n",
    "\n",
    "1. For each raw data, compute the corresponding flat field using lineal interpolation (between `flatfield/0000` and `flatfield/0500`)\n",
    "2. Save each result into different datasets in a single HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise3.py](./solutions/exercise3.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset compression\n",
    "\n",
    "TL;DR: Install [hdf5plugin](https://github.com/silx-kit/hdf5plugin) and `import hdf5plugin`.\n",
    "\n",
    "HDF5 provides dataset compression support.\n",
    "With `h5py` GZIP and LZF compression are available (see [compression-filters](https://docs.h5py.org/en/stable/high/dataset.html#lossless-compression-filters)).\n",
    "Yet, there are many [third-party compression filters for HDF5](https://portal.hdfgroup.org/display/support/Registered+Filter+Plugins) available.\n",
    "\n",
    "[hdf5plugin](https://github.com/silx-kit/hdf5plugin) allows usage of some of those compression filters with `h5py` (blosc, bitshuffle, lz4, FCIDECOMP, ZFP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import hdf5plugin  # Allows to read dataset stored with supported compressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To write compressed datasets, see:\n",
    "\n",
    "- [Group.create_dataset](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset) `chunks`, `compression` and `compression_opts` parameters.\n",
    "- [\"Chunked Storage\" documentation](https://docs.h5py.org/en/stable/high/dataset.html#chunked-storage)\n",
    "- [hdf5plugin documentation](https://github.com/silx-kit/hdf5plugin#documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft and external links\n",
    "\n",
    "A HDF5 file can contain links to Group/Dataset:\n",
    "- within the same file: see [h5py.SoftLink](https://docs.h5py.org/en/stable/high/group.html#soft-links)\n",
    "- in another file: see [h5py.ExternalLink](https://docs.h5py.org/en/stable/high/group.html#external-links)\n",
    "\n",
    "Links can be dangling if the destination does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## External dataset\n",
    "\n",
    "A HDF5 file can contain datasets that are stored in external binary files: See [Group.create_dataset](https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset) `external` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Virtual Dataset (aka. VDS)\n",
    "\n",
    "Virtual dataset allows to map multiple datasets into a single one.\n",
    "Once created it behaves as other datasets.\n",
    "\n",
    "See https://docs.h5py.org/en/stable/vds.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDF5 file locking: A word of caution\n",
    "\n",
    "Do **NOT** open a HDF5 file that is otherwise been written (without caution).\n",
    "\n",
    "By default, HDF5 locks the file even for reading, and other processes cannot open it for writing.\n",
    "This can be an issue, e.g., during acquisition.\n",
    "\n",
    "**WARNING**: With file locking disabled, do not open twice the same file for writing or the file will be corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDF5 file locking can be disabled by setting the `HDF5_USE_FILE_LOCKING` environment variable to `FALSE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "h5file_nolock = h5py.File('my_first_one.h5', mode='r')\n",
    "h5file_nolock.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "h5file_nolock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Warning**: Now `HDF5_USE_FILE_LOCKING` is set for all following calls, this may not be desired.\n",
    "         You might register the existing value of the variable and reset it at the end.\n",
    "         \n",
    "``` python\n",
    "original_state = os.environ.get(\"HDF5_USE_FILE_LOCKING\", None)\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "# Code handling the HDF5 file goes here\n",
    "if original_state is None:\n",
    "    del os.environ[\"HDF5_USE_FILE_LOCKING\"]\n",
    "else:\n",
    "    os.environ[\"HDF5_USE_FILE_LOCKING\"] = original_state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Practical tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- conversion:\n",
    "    - [`silx convert`](http://www.silx.org/doc/silx/latest/applications/convert.html): To convert EDF, or spec files to HDF5\n",
    "\n",
    "- reading/writing HDF5 helpers:\n",
    "    - [`silx.io.dictdump`](http://www.silx.org/doc/silx/latest/modules/io/dictdump.html): `h5todict`, `dicttoh5`\n",
    "    - [`silx.io.utils.h5py_read_dataset`](http://www.silx.org/pub/doc/silx/latest/modules/io/utils.html#silx.io.utils.h5py_read_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[Nexus](https://www.nexusformat.org/) is a data format for neutron, x-ray, and muon science.\n",
    "\n",
    "It aims to be a common data format for scientists for greater collaboration.\n",
    "\n",
    "If you intend to store some data to be shared it can give you a 'standard way' for storing it.\n",
    "\n",
    "The main advantage is to ensure compatibility between your data files and existing softwares (if they respect the nexus format) or from your software to different datasets.\n",
    "\n",
    "* an example on [how to store tomography raw data](http://download.nexusformat.org/doc/html/classes/applications/NXtomo.html?highlight=tomography)\n",
    "* an example to store [tomoraphy application (3D reconstruction)](http://download.nexusformat.org/doc/html/classes/applications/NXtomoproc.html?highlight=tomography)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "[h5py](https://www.h5py.org/) provides access to HDF5 file content from Python through:\n",
    "\n",
    "- [`h5py.File`](https://docs.h5py.org/en/stable/high/file.html) opens a HDF5 file:\n",
    "  - Do not forget the mode: `'r'`, `'a'`, `'w'`.\n",
    "  - Use a `with` statement or do not forget to `close` the file.\n",
    "- [`h5py.Group`](https://docs.h5py.org/en/stable/high/group.html) provides a key-value mapping `dict`-like access to the HDF5 structure.\n",
    "- [`h5py.Dataset`](https://docs.h5py.org/en/stable/high/dataset.html) gives access to data as `numpy.ndarray`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
