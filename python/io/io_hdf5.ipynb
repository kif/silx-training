{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data IO (input/output)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data (used to) come in (too many) different formats:\n",
    "\n",
    "* Specfile\n",
    "* EDF\n",
    "* HDF5\n",
    "\n",
    "And specific detector formats:\n",
    "\n",
    "* MarCCD\n",
    "* Pilatus CBF\n",
    "* Dectris Eiger\n",
    "* â€¦\n",
    "\n",
    "\n",
    "HDF5 is now the standard ESRF data format so we will only focus on it today.\n",
    "\n",
    "Methods for accessing other file format are described in the [io_spec_edf.ipynb](io_spec_edf.ipynb) notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5\n",
    "\n",
    "![hdf_group](images/HDF_logo.png \"HDF group\")\n",
    "\n",
    "## What is hdf5 ?\n",
    "\n",
    "[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) (for Hierarchical Data Format) is a file format to structure and store complex and high volumes of data.\n",
    "\n",
    "## Why hdf5 ?\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Portable file format (Standard exchange format for heterogeneous data)\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "* Free ( & open source)\n",
    "* Adopted by a large number of institutes (NASA, LIGO, ...)\n",
    "* Adopted by most of the synchrotrons (ESRF, SOLEIL, Desy...)\n",
    "\n",
    "**Data can be mostly anything: image, table, graphs, documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes)\n",
    "\n",
    "![hdf5_class_diag](images/hdf5_model.png \"hdf5 class diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by [pyFAI](https://github.com/silx-kit/pyFAI)\n",
    "\n",
    "![hdf5_example](images/hdf5_example.png \"hdf5 example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Useful tools for HDF5\n",
    "\n",
    "* `h5ls`, `h5dump`, `hdfview`\n",
    "```bash\n",
    ">>> h5ls -r my_first_one.h5 \n",
    ">>> /                        Group\n",
    ">>> /data1                   Dataset {100, 100}\n",
    ">>> /group1                  Group\n",
    ">>> /group1/data2            Dataset {100, 100}\n",
    "```\n",
    "\n",
    "* `silx view`\n",
    "\n",
    "```bash\n",
    ">>> pip install silx\n",
    ">>> silx view my_file.h5\n",
    "```\n",
    "\n",
    "* `h5glance`: File browser for jupyter\n",
    "\n",
    "* `h5py`: Access HDF5 files from python\n",
    "\n",
    "==> The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# h5py\n",
    "\n",
    "![h5py book](images/h5py.png \"h5py book\")\n",
    "\n",
    "[h5py](https://www.h5py.org/) is the python binding for accessing hdf5. Originally from [Andrew Collette](http://shop.oreilly.com/product/0636920030249.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "h5py is already available in most ESRF computers.\n",
    "It is cross platform and can be installed using for example:\n",
    "\n",
    "- apt:\n",
    "```bash\n",
    "apt-get install python3-h5py\n",
    "```\n",
    "- pip\n",
    "```bash\n",
    "pip install h5py\n",
    "```\n",
    "- Also available from source code (under MIT license)\n",
    "\n",
    "    * https://github.com/h5py/h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "warning: a major version of h5py has been released lately (3.0). Some code might not be compatible with it yet (end of python2 support, [interface for storing & reading strings has changed](https://docs.h5py.org/en/stable/strings.html)...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "print(h5py.version.version)\n",
    "print(h5py.version.hdf5_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to read an hdf5 file with h5py ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* first open a file using a [File Object](http://docs.h5py.org/en/stable/high/file.html)\n",
    "```\n",
    "h5py.File('myfile.hdf5', opening_mode)\n",
    "```\n",
    "\n",
    "   [opening modes](http://docs.h5py.org/en/stable/high/file.html#opening-creating-files) are:\n",
    "\n",
    "|         |                                                  |\n",
    "|---------|--------------------------------------------------|\n",
    "| r       | Readonly, file must exist                        |\n",
    "| r+      | Read/write, file must exist                      |\n",
    "| w       | Create file, truncate if exists                  |\n",
    "| w- or x | Create file, fail if exists                      |\n",
    "| a       | Read/write if exists, create otherwise (default) |\n",
    "\n",
    "* then you will be able to access your data from the root node. [Groups](http://docs.h5py.org/en/stable/high/group.html) operate as dictionaries.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", h5file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "you can get path to dataset(s) using group keys or silx view or h5glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Remember to close the file\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Context manager\n",
    "\n",
    "* Context manager will allocate and release resources 'automatically' when needed.\n",
    "* Usually used from the `with` statement.\n",
    "\n",
    "so to write safely into a file, instead of having something like above, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Or better, use a context manager\n",
    "# The file is closed for you\n",
    "with h5py.File('data/test.h5', \"r\") as h5file:\n",
    "    print(h5file.keys())\n",
    "    dataset = h5file['/diff_map_0004/data/map']\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## h5py mimics numpy-array\n",
    "\n",
    "The data is read from the file only when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "dataset = h5file['/diff_map_0004/data/map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here we only read metadata from the dataset\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read and apply an operation\n",
    "print(dataset[5, 5, 0:5])\n",
    "print(2 * dataset[0, 5, 0:5])\n",
    "print(2 * dataset[...].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data and store it as a numpy-array\n",
    "# if no copy is done, the data will not be accessible once the file is closed \n",
    "b = dataset[...]\n",
    "c = dataset[0, 0, 0:5]\n",
    "b[0, 0, 0:5] = 0\n",
    "print(b[0, 0, 0:5])\n",
    "print(dataset[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(b[0, 0, 0:5])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to write in a HDF5 file with h5py ?\n",
    "\n",
    "* *there are several ways for writing groups and datasets. Here we will only focus on the 'dictionary' like API.*\n",
    "* http://docs.h5py.org/en/stable/high/group.html\n",
    "* http://docs.h5py.org/en/stable/high/dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "data = numpy.random.random(10000)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# write\n",
    "h5file = h5py.File('my_first_one.h5', mode='w')\n",
    "\n",
    "# write data into a dataset from the root\n",
    "h5file['/data1'] = data\n",
    "\n",
    "# write data into a dataset from group1\n",
    "h5file['/group1/data2'] = data\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same operation with a context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# Notice the mode='w', as 'write'\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file['/data1'] = data\n",
    "\n",
    "    # write data into a dataset from group1\n",
    "    h5file['/group1/data2'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same with a context manager and avoiding the dictionary API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# Notice the mode='w', as 'write'\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file.create_dataset('data1', data=data)\n",
    "\n",
    "    # Or with a functional API\n",
    "    grp1 = h5file.create_group(\"group1\")\n",
    "    grp1.create_dataset(\"data2\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercice: Flat field correction\n",
    "\n",
    "Flat-field correction is a technique used to improve quality in digital imaging.\n",
    "\n",
    "The goal is to normalize images and remove artifacts caused by variations in the pixel-to-pixel sensitivity of the detector and/or by distortions in the optical path. (see https://en.wikipedia.org/wiki/Flat-field_correction)\n",
    "\n",
    "$$ normalized = \\frac{raw - dark}{flat - dark} $$\n",
    "\n",
    "* `normalized`: Image after flat field correction\n",
    "* `raw`: Raw image. It is acquired with the sample.\n",
    "* `flat`: Flat field image. It is the response given out by the detector for a uniform input signal. This image is acquired without the sample.\n",
    "* `dark`: Also named `background` or `dark current`. It is the response given out by the detector when there is no signal. This image is acquired without the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is a function implementing the flat field correction:\n",
    "\n",
    "*note: make sure you execute the cell for this function to be defined*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def flatfield_correction(raw, flat, dark):\n",
    "    \"\"\"\n",
    "    Apply a flat-field correction to a raw data using a flat and a dark.\n",
    "    \"\"\"\n",
    "    # Make sure that the computation is done using float\n",
    "    # to avoid type overflow or loss of precision\n",
    "    raw = raw.astype(numpy.float32)\n",
    "    flat = flat.astype(numpy.float32)\n",
    "    dark = dark.astype(numpy.float32)\n",
    "    # Do the computation\n",
    "    return (raw - dark) / (flat - dark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. Browse the file ``data/ID16B_diatomee.h5``\n",
    "2. Get a single raw dataset, a flat field dataset and a dark image dataset from this file\n",
    "3. Apply the flat field correction\n",
    "4. Save the result into a new HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise1.py](./solutions/exercise1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/ID16B_diatomee.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"data/ID16B_diatomee.h5\", mode=\"r\") as h5s:\n",
    "    pass\n",
    "    # this is a comment\n",
    "\n",
    "    # step1: Read the data\n",
    "\n",
    "    # raw_data_path = ...\n",
    "    # raw_data = ...\n",
    "\n",
    "    # flat_path = ...\n",
    "    # flat = ...\n",
    "\n",
    "    # dark_path = ...\n",
    "    # dark = ...\n",
    "\n",
    "# step2: Compute the result\n",
    "\n",
    "# normalized = flatfield_correction(raw_data, flat, dark)\n",
    "\n",
    "# step3: Save the result\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*note: if you like to plot an image you can use the imshow command !!! the %pylab should be called once before calling the imshow function !!!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "plt.imshow(numpy.random.random((20, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "1. Apply the flat field correction to all raw data available (use the same flat and dark for all the images)\n",
    "2. Save each result into different datasets of the same HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise2.py](./solutions/exercise2.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "From the previous exercise, we can see that the flat field correction was not very good for the last images.\n",
    "\n",
    "Another flat field was acquired at the end of the acquisition.\n",
    "\n",
    "We could use this information to compute a flat field closer to the image we want to normalize. It can be done with a linear interpolation of the flat images by using the name of the image as the interpolation factor (which varies between 0 and 500 in this case).\n",
    "\n",
    "1. For each raw data, compute the corresponding flat field using lineal interpolation (between `flatfield/0000` and `flatfield/0500`)\n",
    "2. Save each result into different datasets in a single HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise3.py](./solutions/exercise3.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5 file locking\n",
    "\n",
    "Do NOT open a HDF5 file without caution that is otherwise been written.When HDF5 file locking is enabled (the default), once a file is opened for reading, other processes cannot open it for writing.\n",
    "\n",
    "This can be an issue, e.g., during acquisition.\n",
    "\n",
    "WARNING: With file locking disabled, do not open twice the same file for writing or the file will be corrupted.To disable file locking, open the HDF5 file this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Set the `HDF5_USE_FILE_LOCKING` environment variable.HDF5 file locking can be disabled by setting the `HDF5_USE_FILE_LOCKING` environment variable to FALSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "h5file_nolock = h5py.File('my_first_one.h5', mode='r')\n",
    "h5file_nolock.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "h5file_nolock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "WARNING: Now `HDF5_USE_FILE_LOCKING` is set for all following calls, this may not be desired.\n",
    "         You might register the existing value of the variable and reset it at the end.\n",
    "         \n",
    "``` python\n",
    "original_lock_state = os.environ.get(\"HDF5_USE_FILE_LOCKING\", None)\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "[...]\n",
    "if original_lock_state is None:\n",
    "    del os.environ[\"HDF5_USE_FILE_LOCKING\"]\n",
    "else:\n",
    "    os.environ[\"HDF5_USE_FILE_LOCKING\"] = original_lock_state\n",
    "```\n",
    "\n",
    "You could also you a context manager for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Preconized libraries according to the use case and the file format.\n",
    "\n",
    "| Formats              | Read            | Write |\n",
    "|----------------------|-----------------|-------|\n",
    "| HDF5                 | silx/h5py       | h5py  |\n",
    "| Specfile             | silx            |       |\n",
    "| EDF                  | silx/fabio      | fabio |\n",
    "| Other raster formats | silx/fabio      | fabio |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Practical tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- conversion\n",
    "    - `fabio-convert`: To convert raster images \n",
    "    - `silx convert`: To convert EDF, or spec files to HDF5\n",
    "\n",
    "- read / writing h5py\n",
    "    - silx.io.dictdump (h5todict, dicttoh5)\n",
    "    - silx.io.utils.h5py_read_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Nexus](https://www.nexusformat.org/) is a data format for neutron, x-ray, and muon science.\n",
    "\n",
    "It aims to be a common data format for scientists for greater collaboration.\n",
    "\n",
    "If you intend to store some data to be shared it can give you a 'standard way' for storing it.\n",
    "\n",
    "The main advantage is to insure compatibility between your data files and existing softwares (if they respect the nexus format) or from your software to different datasets.\n",
    "\n",
    "* an example on [how to store tomography raw data](http://download.nexusformat.org/doc/html/classes/applications/NXtomo.html?highlight=tomography)\n",
    "* an example to store [tomoraphy application (3D reconstruction)](http://download.nexusformat.org/doc/html/classes/applications/NXtomoproc.html?highlight=tomography)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
