{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data IO (input/output)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data come in (too many) different formats:\n",
    "\n",
    "* Specfile\n",
    "* EDF\n",
    "* HDF5\n",
    "\n",
    "And specific detector formats:\n",
    "\n",
    "* MarCCD\n",
    "* Pilatus CBF\n",
    "* Dectris Eiger\n",
    "* …\n",
    "\n",
    "\n",
    "HDF5 is expected to become the standard ESRF data format. Some beamlines have already switched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessing ESRF data\n",
    "\n",
    "## Libraries\n",
    "\n",
    "\n",
    "* h5py\n",
    "    * Access to HDF5 files\n",
    "* FabIO\n",
    "    * Provides access to several image data formats\n",
    "    * Managed by the DAU\n",
    "* silx\n",
    "    * Normalize a way to access any data\n",
    "    * Helper to simplify the transition to HDF5\n",
    "    * `silx view` to show the file structure\n",
    "    * Also provides data processing functions\n",
    "    * Managed by the DAU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessing ESRF data\n",
    "\n",
    "## Libraries\n",
    "\n",
    "\n",
    "Those are already available for most ESRF computers."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apt-get install python3-silx python3-fabio python3-h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross platform (available for Windows, Linux, Mac OS X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install silx fabio h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also available from source code (under MIT license)\n",
    "\n",
    "* https://github.com/silx-kit/silx\n",
    "* https://github.com/silx-kit/fabio\n",
    "* https://github.com/h5py/h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spec files\n",
    "\n",
    "* Text format from Spec sequencer\n",
    "* Contains evolution of measurments and instruments during a scan\n",
    "* We do not recommand to use this format anymore\n",
    "* `silx` provides a HDF5-like read access to Spec files\n",
    "\n",
    "### Spec compatibility\n",
    "\n",
    "* PyMCA was previously often used as a Python library to read Spec files\n",
    "* Now prefer using silx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of\n",
    "from PyMca5.PyMca import specfilewrapper\n",
    "# prefer using\n",
    "from silx.io import specfilewrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to read a spec file\n",
    "\n",
    "An example is given later in [spec files using silx](#Read-Spec-file-as-an-HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EDF files\n",
    "\n",
    "\n",
    "* ESRF data format\n",
    "* It contains\n",
    "    * Header containing various informations\n",
    "    * 1D/2D/3D array of float/integer\n",
    "    * Multi-frames (more than one image in a single file)\n",
    "    * Often used as file series\n",
    "* Library\n",
    "    * Use `fabio`\n",
    "    * `silx` provides a HDF5-like read access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read a single EDF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "image = fabio.open(\"data/medipix.edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the data as a numpy array\n",
    "print(image.data)\n",
    "# Here is the header as key-value dictionary\n",
    "print(image.header.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to use a context manager\n",
    "with fabio.open(\"data/medipix.edf\") as image:\n",
    "    print(image.header[\"dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read a multi-frame EDF image\n",
    "\n",
    "A file containing many frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "with fabio.open(\"data/ID16B_diatomee.edf\") as image:\n",
    "\n",
    "    print(\"Nb frames: %d\" % image.nframes)\n",
    "\n",
    "    for frame in image.frames():\n",
    "\n",
    "        average = frame.data.mean()\n",
    "        \n",
    "        message = \"Frame ID: %d    Data average: %0.2f\"\n",
    "        print(message % (frame.index, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read a file-series of EDF image\n",
    "\n",
    "A file-series is compound by many files that have to be iterated, and may contain many frames. `open_series` can be used.\n",
    "\n",
    "- http://www.silx.org/doc/fabio/latest/getting_started.html#fabio-file-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "with fabio.open_series(first_filename=\"data/ID19_D2H2T2_0000.edf\") as series:\n",
    "\n",
    "    print(\"Nb frames: %d\" % series.nframes)\n",
    "\n",
    "    for frame in series.frames():\n",
    "\n",
    "        average = frame.data.mean()\n",
    "\n",
    "        message = \"Filename: %s    Frame ID: %d    Data average: %0.2f\"\n",
    "        print(message % (frame.file_container.filename, frame.index, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Write an EDF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import fabio\n",
    "\n",
    "image = numpy.random.rand(10, 10)\n",
    "metadata = {'pixel_size': '0.2'}\n",
    "\n",
    "image = fabio.edfimage.EdfImage(data=image, header=metadata)\n",
    "image.write('edf_writing_example.edf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other formats using FabIO\n",
    "\n",
    "### Reading other formats\n",
    "\n",
    "FabIO supports image formats from most manufacturers: \n",
    "Mar, Rayonix, Bruker, Dectris, ADSC, Rigaku, Oxford, General Electric…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "pilatus_image    = fabio.open('filename.cbf')\n",
    "marccd_image     = fabio.open('filename.mccd')\n",
    "\n",
    "tiff_image       = fabio.open('filename.tif')\n",
    "fit2d_mask_image = fabio.open('filename.msk')\n",
    "jpeg_image       = fabio.open('filename.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5\n",
    "\n",
    "## HDF5 introduction\n",
    "\n",
    "HDF5 (for Hierarchical Data Format) is a file format to structure and store data for high volume and complex data\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Standard exchange format for heterogeneous data\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "\n",
    "Data can be mostly anything: image, table, graphs, documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes)\n",
    "\n",
    "<img src=\"images/hdf5_model.png\" style=\"height:50%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by pyFAI\n",
    "\n",
    "<img src=\"images/hdf5_example.png\" style=\"height:50%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read an HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", h5file['/'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# Here we only read metadata from the dataset\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to close the file\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or better, use a context manager\n",
    "# The file is closed for you\n",
    "with h5py.File('data/test.h5', \"r\") as h5file:\n",
    "    print(h5file['/'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDF5 mimics numpy-array\n",
    "\n",
    "The data are reached from the file only when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "dataset = h5file['/diff_map_0004/data/map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read and apply an operation\n",
    "print(dataset[5, 5, 0:5])\n",
    "print(2 * dataset[0, 5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data and store it as a numpy-array\n",
    "b = dataset[...]\n",
    "b[0, 0, 0:5] = 0\n",
    "print(dataset[0, 0, 0:5])\n",
    "print(b[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Write an HDF5\n",
    "\n",
    "* http://docs.h5py.org/en/stable/high/group.html\n",
    "* http://docs.h5py.org/en/stable/high/dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create a 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# Notice the mode='w', as 'write'\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file['/data1'] = data\n",
    "\n",
    "    # write data into a dataset from group1\n",
    "    h5file['/group1/data2'] = data\n",
    "\n",
    "    # Or with a functional API\n",
    "    g = h5file.create_group(\"/group2\")\n",
    "    g.create_dataset(\"data3\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Usefull tools for HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r my_first_one.h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"my_first_one.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `h5py`: Connector to HDF5 files\n",
    "* `silx view`: Qt file browser\n",
    "* `h5glance`: File browser for jupyter\n",
    "\n",
    "The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Module `silx.io`\n",
    "\n",
    "* Try to simplify the transition to HDF5\n",
    "    * h5py-like API\n",
    "    * Single way to access to Spec/EDF/HDF5 files\n",
    "    * Based on NeXus specifications http://www.nexusformat.org/\n",
    "* Read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General mapping from Spec file\n",
    "\n",
    "Silx can expose spec file with an HDF5-like mapping.\n",
    "\n",
    "![mapping_spec](images/spech5_arrows.png \"hdf5-like mapping for spec files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General mapping from EDF image\n",
    "\n",
    "Silx can expose EDF file (or any support formats from `fabio`) with a HDF5-like mapping.\n",
    "\n",
    "![mapping_spec](images/fabioh5_arrows.png \"hdf5-like mapping for EDF files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Display the mapping with tools\n",
    "\n",
    "* `silx view` a command line Qt program.\n",
    "* `silx.io.utils.h5ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "import silx.io.utils\n",
    "\n",
    "with silx.io.open('data/oleg.dat') as h5file:\n",
    "    string = silx.io.utils.h5ls(h5file)\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read Spec file as an HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import silx.io\n",
    "data = silx.io.open('data/oleg.dat')\n",
    "\n",
    "# Available scans\n",
    "print(\"First childs:\", data['/'].keys())\n",
    "\n",
    "# Available measurements from the scan 94.1\n",
    "print(\"Containt of measurement:\", data['/94.1/measurement'].keys())\n",
    "\n",
    "# Get data from measurement\n",
    "epoch = data['/94.1/measurement/Epoch']\n",
    "bpmi = data['/94.1/measurement/bpmi']\n",
    "for t, data in zip(epoch, bpmi):\n",
    "    t = time.strftime(\"%X\", time.gmtime(t))\n",
    "    print(\"%s   BPMi: %0.4e\" % (t, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For more information and examples you can read the silx IO tutorial: https://github.com/silx-kit/silx-training/blob/master/silx/io/io.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read EDF image as an HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "data = silx.io.open('data/ID16B_diatomee.edf')\n",
    "\n",
    "# Access to the frames\n",
    "frames = data['/scan_0/instrument/detector_0/data']\n",
    "len(frames)  # number of frames\n",
    "frames[0]    # first frame\n",
    "print(\"Number of frames:\", len(frames))\n",
    "print(\"Size of an image:\", frames[0].shape)\n",
    "\n",
    "# Access to motors, monitor, timestanp\n",
    "srot = data['scan_0/instrument/positioners/srot'][...]\n",
    "mon = data['scan_0/measurement/mon'][...]\n",
    "timestamp = data['scan_0/instrument/detector_0/others/time_of_day'][...]\n",
    "for t, s, m in zip(timestamp, srot, mon):\n",
    "    t = time.strftime(\"%X\", time.gmtime(t))\n",
    "    message = \"%s   Rot:% 5.1fdeg   Monitor: %0.2f\"\n",
    "    print(message % (t, s, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read HDF5 using silx\n",
    "\n",
    "For conveniance, ``silx`` also provides the h5py API for HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "h5file = silx.io.open('data/test.h5')\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", h5file['/'].keys())\n",
    "\n",
    "# reaching a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# using size and types to not read the full stored data\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Convert tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- `fabio-convert`: To convert raster images \n",
    "- `silx convert`: To convert EDF, or spec files to HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercice: Flat field correction\n",
    "\n",
    "Flat-field correction is a technique used to improve quality in digital imaging.\n",
    "\n",
    "The goal is to normalize images caused by variations in the pixel-to-pixel sensitivity of the detector and/or by distortions in the optical path.\n",
    "\n",
    "$ normalized = {{raw - dark}\\over{flat - dark}}$\n",
    "\n",
    "* `normalized`: Image after flat field correction\n",
    "* `raw`: Raw image. That's an acquisition from a sample.\n",
    "* `flat`: Flat field image. Is is the response given out by the detector for a uniform input signal. It is acquired without the sample.\n",
    "* `dark`: Also named `background` or `dark current`. It is the response given out by the detector when there is no signal. The image is acquired without the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercice: Implementation with EDF files\n",
    "\n",
    "Here is helper already provided to compute the flat field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we provide some helpers\n",
    "\n",
    "import fabio\n",
    "import numpy\n",
    "\n",
    "def flatfield_correction(raw, flat, dark):\n",
    "    \"\"\"\n",
    "    Apply a flat-field correction to a raw data using a flat and a dark.\n",
    "    \"\"\"\n",
    "    # Make sure that the computation is done using float\n",
    "    # to avoid type overflow or lose of precision\n",
    "    raw = raw.astype(numpy.float32)\n",
    "    flat = flat.astype(numpy.float32)\n",
    "    dark = dark.astype(numpy.float32)\n",
    "    # To the computation\n",
    "    return (raw - dark) / (flat - dark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a `matplotlib` function to help to display data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowmany(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Dispaly as image all array provided as argument.\n",
    "    \n",
    "    The image title is defined using the argument name.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    if len(kwargs) == 0:\n",
    "        import collections\n",
    "        kwargs = collections.OrderedDict()\n",
    "    for i, arg in enumerate(args):\n",
    "        if isinstance(arg, dict):\n",
    "            kwargs.update(arg)\n",
    "        else:\n",
    "            kwargs[\"arg\" + i]\n",
    "\n",
    "    fig = pyplot.figure()\n",
    "    columns = 3\n",
    "    nbrows = len(kwargs) // columns + 1\n",
    "    nbcols = len(kwargs) // nbrows\n",
    "    for i, (key, value) in enumerate(kwargs.items()):\n",
    "        a = fig.add_subplot(nbrows, nbcols, i + 1)\n",
    "        imgplot = plt.imshow(value)\n",
    "        a.set_title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation of a flat field correction applied on a single EDF files.\n",
    "\n",
    "The sample is a diatom, an unicellular algae inserted on a needle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "with fabio.open(\"data/ID16_diatomee/dark.edf\") as image:\n",
    "    dark = image.data\n",
    "with fabio.open(\"data/ID16_diatomee/flat.edf\") as image:\n",
    "    flat = image.data\n",
    "with fabio.open(\"data/ID16_diatomee/data.edf\") as image:\n",
    "    raw = image.data\n",
    "\n",
    "# Compute the result\n",
    "\n",
    "normalized = flatfield_correction(raw, flat, dark)\n",
    "\n",
    "# Save the result\n",
    "\n",
    "image = fabio.edfimage.EdfImage(data=normalized)\n",
    "image.save(\"result.edf\")\n",
    "\n",
    "# Check the saved result\n",
    "\n",
    "with fabio.open(\"result.edf\") as image:\n",
    "    saved = image.data\n",
    "imshowmany(Before=raw, After=saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. Browse the file ``data/ID16B_diatomee.h5``\n",
    "2. Reach a single raw data, a flat field and a dark image from this file\n",
    "3. Apply the flat field correction\n",
    "4. Save the result into a new HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise1.py](./solutions/exercise1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"data/ID16B_diatomee.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Read the data\n",
    "\n",
    "...\n",
    "\n",
    "# Compute the result\n",
    "\n",
    "normalized = flatfield_correction(raw, flat, dark)\n",
    "\n",
    "# Save the result\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "1. Apply the flat field correction to all the raw data available (use the same flat and dark for all the images)\n",
    "2. Save each result into a new dataset alltogether with in a single HDF5 file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise2.py](./solutions/exercise2.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "From the previous exercise, we can see that the flat field correction was not very good for the last images.\n",
    "\n",
    "But another flat field was acquired at the end of the acquisition.\n",
    "\n",
    "We could use this information to compute a flat field closer to the image we want to normalize. It can be done using a linear interpolation between 0 and 500 using the name of the image.\n",
    "\n",
    "1. For each raw data to normalize, compute flat field using a lineal interpolation (between `flatfield/0000` and `flatfield/0500`)\n",
    "2. Save each result in a file\n",
    "\n",
    "If you are stuck, the solution is provided in the file [solutions/exercise3.py](./solutions/exercise3.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Preconized libraries according to the use case and the file format.\n",
    "\n",
    "| Formats              | Read            | Write |\n",
    "|----------------------|-----------------|-------|\n",
    "| HDF5                 | silx/h5py       | h5py  |\n",
    "| Specfile             | silx            |       |\n",
    "| EDF                  | silx/fabio      | fabio |\n",
    "| Other raster formats | silx/fabio      | fabio |"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
