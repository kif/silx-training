{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "io -> Input/output\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data come in (too many) different formats:\n",
    "\n",
    "* Specfile\n",
    "* EDF\n",
    "* HDF5\n",
    "\n",
    "and specific detector formats:\n",
    "\n",
    "* MarCCD\n",
    "* Pilatus CBF\n",
    "* Dectris Eiger\n",
    "* …\n",
    "\n",
    "\n",
    "HDF5 is expected to become the standard ESRF data format. Some beamlines have already switched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessing ESRF data\n",
    "\n",
    "## Libraries\n",
    "\n",
    "\n",
    "* h5py\n",
    "    * Access to HDF5 files\n",
    "* FabIO\n",
    "    * Provides access to several image data formats\n",
    "    * Developed as part of the Fable project, initially an ID11 development.\n",
    "    * Managed by the DAU\n",
    "* silx\n",
    "    * Started in 2015\n",
    "    * Will provide input/output for PyMCA\n",
    "    * Also provides fitting, image processing, plotting, a set of widgets…\n",
    "    * Managed by the DAU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Those are already available for most ESRF computers\n",
    "\n",
    "```bash\n",
    ">>> apt-get install python3-silx python3-fabio python3-h5py\n",
    "```\n",
    "\n",
    "Cross platform (Available for Windows, Linux, Mac OS X)\n",
    "```bash\n",
    ">>> pip install silx fabio h5py\n",
    "```\n",
    "\n",
    "\n",
    "Also available from source code (under MIT license)\n",
    "\n",
    "* https://github.com/silx-kit/silx\n",
    "* https://github.com/silx-kit/fabio\n",
    "* https://github.com/h5py/h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spec files\n",
    "\n",
    "* text format from Spec\n",
    "* contains evolution of measurments and instruments during a scan\n",
    "* we do not recommand to use this format anymore\n",
    "* silx provides a HDF5-like read access to Spec files\n",
    "\n",
    "### Spec compatibility\n",
    "\n",
    "* PyMCA was previously often used as a Python library to read Spec files\n",
    "* now prefer using silx\n",
    "\n",
    "```python\n",
    "# instead of\n",
    "from PyMca5.PyMca import specfilewrapper\n",
    "\n",
    "# prefer using\n",
    "from silx.io import specfilewrapper\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to read a spec file\n",
    "\n",
    "An example is given later in [spec files using silx](#Reading-spec-files-using-silx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EDF files\n",
    "\n",
    "\n",
    "* ESRF data format\n",
    "* It contains\n",
    "    * 1D/2D/3D array of float/integer\n",
    "    * Header containing various informations\n",
    "    * Multi-frames (more than one image in a single file)\n",
    "    * Often used as file series\n",
    "* Library\n",
    "    * Use FabIO\n",
    "    * silx provides a HDF5-like read access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### reading EDF files using fabIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "image = fabio.open(\"data/medipix.edf\")\n",
    "\n",
    "# here is the data as a numpy array\n",
    "print(image.data)\n",
    "\n",
    "# here is the header as key-value dictionary\n",
    "print(image.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### writing files using fabIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import fabio\n",
    "\n",
    "image = numpy.random.rand(10, 10)\n",
    "metadata = {'pixel_size': '0.2'}\n",
    "\n",
    "image = fabio.edfimage.EdfImage(data=image, header=metadata)\n",
    "image.write('edf_writing_example.edf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other formats using FabIO\n",
    "\n",
    "#### Reading other formats\n",
    "\n",
    "FabIO supports image formats from most manufacturers: \n",
    "Mar, Rayonix, Bruker, Dectris, ADSC, Rigaku, Oxford, General Electric…\n",
    "\n",
    "```python\n",
    "import fabio\n",
    "\n",
    "pilatus_image    = fabio.open('filename.cbf')\n",
    "marccd_image     = fabio.open('filename.mccd')\n",
    "\n",
    "tiff_image       = fabio.open('filename.tif')\n",
    "fit2d_mask_image = fabio.open('filename.msk')\n",
    "jpeg_image       = fabio.open('filename.jpg')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### File conversion\n",
    "\n",
    "Using FabIO you can directly convert data to an other format \n",
    "\n",
    "```python\n",
    "import fabio\n",
    "image = fabio.open('data/medipix.edf')\n",
    "image = image.convert('tif')\n",
    "image.save('filename.tif')\n",
    "```\n",
    "(you can also use the command-line fabio-convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5\n",
    "\n",
    "## HDF5 introduction\n",
    "\n",
    "HDF5 (for Hierarchical Data Format) is a file format to structure and store data for high volume and complex data\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Standard exchange format for heterogeneous data\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "\n",
    "Data can be mostly anything: image, table, graphs, documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes)\n",
    "\n",
    "![hdf5_class_diag](images/hdf5_model.png \"hdf5 class diagram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by pyFAI\n",
    "\n",
    "![hdf5_example](images/hdf5_example.png \"hdf5 example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we read a specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5')\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", list(h5file['/'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# reaching a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# using size and types to not read the full stored data\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "datasets mimics numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# read and apply the operation\n",
    "print(dataset[5, 5, 0:5])\n",
    "print(2 * dataset[0, 5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data and store it as a numpy-array\n",
    "b = dataset[...]\n",
    "b[0, 0, 0:5] = 0\n",
    "print(dataset[0, 0, 0:5])\n",
    "print(b[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## h5py write example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "data = numpy.arange(10000.0)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# write\n",
    "h5file = h5py.File('my_first_one.h5', mode='w')\n",
    "\n",
    "# write data into a dataset from the root\n",
    "h5file['/data1'] = data\n",
    "\n",
    "# write data into a dataset from group1\n",
    "h5file['/group1/data2'] = data\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Usefull tools for HDF5\n",
    "\n",
    "* h5ls, h5dump, hdfview\n",
    "```bash\n",
    ">>> h5ls -r my_first_one.h5 \n",
    ">>> /                        Group\n",
    ">>> /data1                   Dataset {100, 100}\n",
    ">>> /group1                  Group\n",
    ">>> /group1/data2            Dataset {100, 100}\n",
    "```\n",
    "\n",
    "* h5py\n",
    "* silx\n",
    "* silx view\n",
    "\n",
    "==> The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# silx io\n",
    "\n",
    "* Try to simplify the transition to HDF5\n",
    "    * Provide a h5py-like API on top of format used at ESRF\n",
    "    * Single way to access to Spec/EDF/HDF5 files\n",
    "    * Based on NeXus specifications http://www.nexusformat.org/\n",
    "* Read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Read HDF5 using silx\n",
    "\n",
    "For conveniance, ``silx`` also provides the h5py API for HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "\n",
    "h5file = silx.io.open('data/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print available names at the first level\n",
    "print(\"First children:\", list(h5file['/'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# reaching a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# using size and types to not read the full stored data\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## spec files using silx\n",
    "\n",
    "Silx can also expose spec file with a HDF5-like mapping\n",
    "\n",
    "### HDF5-like mapping  (given for general information)\n",
    "\n",
    "![mapping_spec](images/spech5_arrows.png \"hdf5-like mapping for spec files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading spec files using silx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "data = silx.io.open('data/oleg.dat')\n",
    "\n",
    "# print available scans\n",
    "print(\"First childs:\", data['/'].keys())\n",
    "\n",
    "# print available measurements from the scan 94.1\n",
    "print(\"Containt of measurement:\", data['/94.1/measurement'].keys())\n",
    "\n",
    "# get data from measurement\n",
    "xdata = data['/94.1/measurement/Epoch']\n",
    "ydata = data['/94.1/measurement/bpmi']\n",
    "for row in zip(xdata, ydata):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For more information and examples you can read the silx IO tutorial: https://github.com/silx-kit/silx-training/blob/master/silx/io/io.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EDF files using silx\n",
    "\n",
    "Silx can also expose spec file with a HDF5-like mapping\n",
    "\n",
    "### HDF5-like mapping (given for general information)\n",
    "\n",
    "![mapping_spec](images/fabioh5_arrows.png \"hdf5-like mapping for EDF files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Read EDF file using silx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "data = silx.io.open('data/ID16B_diatomee.edf')\n",
    "\n",
    "# Access to the frames\n",
    "frames = data['/scan_0/instrument/detector_0/data']\n",
    "len(frames)  # number of frames\n",
    "frames[0]    # first frame\n",
    "print(\"Number of frames:\", len(frames))\n",
    "print(\"Size of an image:\", frames[0].shape)\n",
    "\n",
    "# Access to motors, monitor, timestanp\n",
    "srot = data['scan_0/instrument/positioners/srot'][...]\n",
    "mon = data['scan_0/measurement/mon'][...]\n",
    "timestamp = data['scan_0/instrument/detector_0/others/time_of_day'][...]\n",
    "for row in zip(timestamp, srot, mon):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Silx Tools / utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### silx.io.utils.h5ls\n",
    "List tree contains\n",
    "`h5ls` allow you to display the tree contained into an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "import silx.io.utils\n",
    "\n",
    "h5file = silx.io.open('data/test.h5')\n",
    "\n",
    "string = silx.io.utils.h5ls(h5file)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### silx.io.convert.write_to_h5\n",
    "\n",
    "Convert spec file to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from silx.io.convert import write_to_h5\n",
    "\n",
    "write_to_h5('data/oleg.dat', 'oleg.h5', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls -al oleg.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "\n",
    "1. Read the EDF file ``medipix.edf``.\n",
    "2. Data processing. The goal of the processing is to clamp the pixels values to a new range of values ([10%, 90%] of the existing one). To do so:\n",
    "\n",
    "    * Create a mask to detect pixel which are below 10% \n",
    "    * With the above mask, set the affected pixels to the 10% 'low value'.\n",
    "    * do the same for value above 90%\n",
    "    * create the mask of all the modify pixel\n",
    "\n",
    "3. Store the source, the mask of changed pixels and the result inside ``process.h5``, as below.\n",
    "\n",
    "   ![Output file structure](images/exercise-result.png)\n",
    "\n",
    "4. Load ``process.h5`` and list the root content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load data/medipix.edf\n",
    "# ...\n",
    "\n",
    "# Process the data\n",
    "# ...\n",
    "\n",
    "# Save data into a new file (process.h5)\n",
    "# ...\n",
    "\n",
    "# Load process.h5 and list the root content\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data/medipix.edf\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.load_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# process data\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.process_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.save_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# list root\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.list_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# result\n",
    "import exercicesolution\n",
    "raw_data, proc_data, mask = exercicesolution.solution(\"data/medipix.edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imshow(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imshow(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Preconized library according to the use case and the file format.\n",
    "\n",
    "| Formats              | Read       | Write |\n",
    "|----------------------|------------|-------|\n",
    "| HDF5                 | silx/h5py  | h5py  |\n",
    "| Specfile             | silx       |       |\n",
    "| EDF multiframe       | silx/fabio | fabio |\n",
    "| EDF                  | fabio      | fabio |\n",
    "| Other raster formats | fabio      | fabio |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
