{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data IO (input/output)\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "ESRF data come in (too many) different formats:\n",
    "\n",
    "* Specfile\n",
    "* EDF\n",
    "* HDF5\n",
    "\n",
    "And specific detector formats:\n",
    "\n",
    "* MarCCD\n",
    "* Pilatus CBF\n",
    "* Dectris Eiger\n",
    "* …\n",
    "\n",
    "\n",
    "HDF5 is expected to become the standard ESRF data format. Some beamlines have already switched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessing ESRF data\n",
    "\n",
    "## Libraries\n",
    "\n",
    "\n",
    "* h5py\n",
    "    * Access to HDF5 files\n",
    "* FabIO\n",
    "    * Provides access to several image data formats\n",
    "    * Managed by the DAU\n",
    "* silx\n",
    "    * Normalize a way to access any data\n",
    "    * Helper to simplify the transition to HDF5\n",
    "    * `silx view` to show the file structure\n",
    "    * Also provides data processing functions\n",
    "    * Managed by the DAU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessing ESRF data\n",
    "\n",
    "## Libraries\n",
    "\n",
    "\n",
    "Those are already available for most ESRF computers."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apt-get install python3-silx python3-fabio python3-h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross platform (available for Windows, Linux, Mac OS X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install silx fabio h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also available from source code (under MIT license)\n",
    "\n",
    "* https://github.com/silx-kit/silx\n",
    "* https://github.com/silx-kit/fabio\n",
    "* https://github.com/h5py/h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spec files\n",
    "\n",
    "* Text format from Spec sequencer\n",
    "* Contains evolution of measurments and instruments during a scan\n",
    "* We do not recommand to use this format anymore\n",
    "* `silx` provides a HDF5-like read access to Spec files\n",
    "\n",
    "### Spec compatibility\n",
    "\n",
    "* PyMCA was previously often used as a Python library to read Spec files\n",
    "* Now prefer using silx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of\n",
    "from PyMca5.PyMca import specfilewrapper\n",
    "# prefer using\n",
    "from silx.io import specfilewrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to read a spec file\n",
    "\n",
    "An example is given later in [spec files using silx](#Reading-spec-files-using-silx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EDF files\n",
    "\n",
    "\n",
    "* ESRF data format\n",
    "* It contains\n",
    "    * Header containing various informations\n",
    "    * 1D/2D/3D array of float/integer\n",
    "    * Multi-frames (more than one image in a single file)\n",
    "    * Often used as file series\n",
    "* Library\n",
    "    * Use `fabio`\n",
    "    * `silx` provides a HDF5-like read access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read a single EDF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "image = fabio.open(\"data/medipix.edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the data as a numpy array\n",
    "print(image.data)\n",
    "# Here is the header as key-value dictionary\n",
    "print(image.header.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to use a context manager\n",
    "with fabio.open(\"data/medipix.edf\") as image:\n",
    "    print(image.header[\"dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read a multi-frame EDF image\n",
    "\n",
    "A file containing many frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "with fabio.open(\"data/ID16B_diatomee.edf\") as image:\n",
    "\n",
    "    print(\"Nb frames: %d\" % image.nframes)\n",
    "\n",
    "    for frame in image.frames():\n",
    "\n",
    "        average = frame.data.mean()\n",
    "        \n",
    "        message = \"Frame ID: %d    Data average: %0.2f\"\n",
    "        print(message % (frame.index, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a file-series of EDF image\n",
    "\n",
    "Many files that have to be iterated, and may contain many frames.\n",
    "\n",
    "- http://www.silx.org/doc/fabio/latest/getting_started.html#fabio-file-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "with fabio.open_series(first_filename=\"data/ID19_D2H2T2_0000.edf\") as series:\n",
    "\n",
    "    print(\"Nb frames: %d\" % series.nframes)\n",
    "\n",
    "    for frame in series.frames():\n",
    "\n",
    "        average = frame.data.mean()\n",
    "\n",
    "        message = \"Filename: %s    Frame ID: %d    Data average: %0.2f\"\n",
    "        print(message % (frame.file_container.filename, frame.index, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Write an EDF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import fabio\n",
    "\n",
    "image = numpy.random.rand(10, 10)\n",
    "metadata = {'pixel_size': '0.2'}\n",
    "\n",
    "image = fabio.edfimage.EdfImage(data=image, header=metadata)\n",
    "image.write('edf_writing_example.edf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other formats using FabIO\n",
    "\n",
    "### Reading other formats\n",
    "\n",
    "FabIO supports image formats from most manufacturers: \n",
    "Mar, Rayonix, Bruker, Dectris, ADSC, Rigaku, Oxford, General Electric…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "\n",
    "pilatus_image    = fabio.open('filename.cbf')\n",
    "marccd_image     = fabio.open('filename.mccd')\n",
    "\n",
    "tiff_image       = fabio.open('filename.tif')\n",
    "fit2d_mask_image = fabio.open('filename.msk')\n",
    "jpeg_image       = fabio.open('filename.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## File conversion\n",
    "\n",
    "Using FabIO you can directly convert data to an other format.\n",
    "\n",
    "You can also use the command-line `fabio-convert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fabio\n",
    "image = fabio.open('data/medipix.edf')\n",
    "image = image.convert('tif')\n",
    "image.save('filename.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDF5\n",
    "\n",
    "## HDF5 introduction\n",
    "\n",
    "HDF5 (for Hierarchical Data Format) is a file format to structure and store data for high volume and complex data\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Standard exchange format for heterogeneous data\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "\n",
    "Data can be mostly anything: image, table, graphs, documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes)\n",
    "\n",
    "<img src=\"images/hdf5_model.png\" style=\"height:50%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## HDF5 example\n",
    "\n",
    "Here is an example of the file generated by pyFAI\n",
    "\n",
    "<img src=\"images/hdf5_example.png\" style=\"height:50%;margin-left:auto;margin-right:auto;padding:0em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read an HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", h5file['/'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# Here we only read metadata from the dataset\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to close the file\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or better to use a context manager\n",
    "with h5py.File('data/test.h5', \"r\") as h5file:\n",
    "    print(h5file['/'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HDF5 mimics numpy-array\n",
    "\n",
    "The data are reached from the file only when it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5file = h5py.File('data/test.h5', \"r\")\n",
    "dataset = h5file['/diff_map_0004/data/map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104.14766  103.352615 103.01642  103.24001  103.27751 ]\n",
      "[205.95827 206.2795  206.5441  206.48112 206.46625]\n"
     ]
    }
   ],
   "source": [
    "# Read and apply an operation\n",
    "print(dataset[5, 5, 0:5])\n",
    "print(2 * dataset[0, 5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# copy the data and store it as a numpy-array\n",
    "b = dataset[...]\n",
    "b[0, 0, 0:5] = 0\n",
    "print(dataset[0, 0, 0:5])\n",
    "print(b[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Write an HDF5\n",
    "\n",
    "* http://docs.h5py.org/en/stable/high/group.html\n",
    "* http://docs.h5py.org/en/stable/high/dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "# Create a 2D data\n",
    "data = numpy.arange(100 * 100)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# Notice the mode='w', as 'write'\n",
    "with h5py.File('my_first_one.h5', mode='w') as h5file:\n",
    "\n",
    "    # write data into a dataset from the root\n",
    "    h5file['/data1'] = data\n",
    "\n",
    "    # write data into a dataset from group1\n",
    "    h5file['/group1/data2'] = data\n",
    "\n",
    "    # Or with a functional API\n",
    "    g = h5file.create_group(\"/group2\")\n",
    "    g.create_dataset(\"data3\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Usefull tools for HDF5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> h5ls -r my_first_one.h5 \n",
    "/                        Group\n",
    "/data1                   Dataset {100, 100}\n",
    "/group1                  Group\n",
    "/group1/data2            Dataset {100, 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* h5py\n",
    "* silx\n",
    "* silx view\n",
    "\n",
    "The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Module `silx.io`\n",
    "\n",
    "* Try to simplify the transition to HDF5\n",
    "    * h5py-like API\n",
    "    * Single way to access to Spec/EDF/HDF5 files\n",
    "    * Based on NeXus specifications http://www.nexusformat.org/\n",
    "* Read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General mapping from Spec file\n",
    "\n",
    "Silx can expose spec file with an HDF5-like mapping.\n",
    "\n",
    "![mapping_spec](images/spech5_arrows.png \"hdf5-like mapping for spec files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General mapping from EDF image\n",
    "\n",
    "Silx can expose EDF file (or any support formats from `fabio`) with a HDF5-like mapping\n",
    "\n",
    "![mapping_spec](images/fabioh5_arrows.png \"hdf5-like mapping for EDF files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Display the mapping with tools\n",
    "\n",
    "* `silx view` a command line Qt program.\n",
    "* `silx.io.utils.h5ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "import silx.io.utils\n",
    "\n",
    "with silx.io.open('data/test.h5') as h5file:\n",
    "\n",
    "    string = silx.io.utils.h5ls(h5file)\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read HDF5 using silx\n",
    "\n",
    "For conveniance, ``silx`` also provides the h5py API for HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First children: <KeysViewHDF5 ['diff_map_0000', 'diff_map_0001', 'diff_map_0002', 'diff_map_0003', 'diff_map_0004']>\n",
      "Dataset: (29, 78, 100) 226200 float32\n"
     ]
    }
   ],
   "source": [
    "import silx.io\n",
    "h5file = silx.io.open('data/test.h5')\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", h5file['/'].keys())\n",
    "\n",
    "# reaching a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# using size and types to not read the full stored data\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read Spec file as an HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import silx.io\n",
    "data = silx.io.open('data/oleg.dat')\n",
    "\n",
    "# Available scans\n",
    "print(\"First childs:\", data['/'].keys())\n",
    "\n",
    "# Available measurements from the scan 94.1\n",
    "print(\"Containt of measurement:\", data['/94.1/measurement'].keys())\n",
    "\n",
    "# Get data from measurement\n",
    "epoch = data['/94.1/measurement/Epoch']\n",
    "bpmi = data['/94.1/measurement/bpmi']\n",
    "for t, data in zip(epoch, bpmi):\n",
    "    t = time.strftime(\"%X\", time.gmtime(t))\n",
    "    print(\"%s   BPMi: %0.4e\" % (t, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For more information and examples you can read the silx IO tutorial: https://github.com/silx-kit/silx-training/blob/master/silx/io/io.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read EDF image as an HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "data = silx.io.open('data/ID16B_diatomee.edf')\n",
    "\n",
    "# Access to the frames\n",
    "frames = data['/scan_0/instrument/detector_0/data']\n",
    "len(frames)  # number of frames\n",
    "frames[0]    # first frame\n",
    "print(\"Number of frames:\", len(frames))\n",
    "print(\"Size of an image:\", frames[0].shape)\n",
    "\n",
    "# Access to motors, monitor, timestanp\n",
    "srot = data['scan_0/instrument/positioners/srot'][...]\n",
    "mon = data['scan_0/measurement/mon'][...]\n",
    "timestamp = data['scan_0/instrument/detector_0/others/time_of_day'][...]\n",
    "for t, s, m in zip(timestamp, srot, mon):\n",
    "    t = time.strftime(\"%X\", time.gmtime(t))\n",
    "    message = \"%s   Rot:% 5.1fdeg   Monitor: %0.2f\"\n",
    "    print(message % (t, s, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Convert tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### silx.io.convert.write_to_h5\n",
    "\n",
    "Convert spec file to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from silx.io.convert import write_to_h5\n",
    "\n",
    "write_to_h5('data/oleg.dat', 'oleg.h5', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls -al oleg.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "\n",
    "1. Read the EDF file ``medipix.edf``.\n",
    "2. Data processing. The goal of the processing is to clamp the pixels values to a new range of values ([10%, 90%] of the existing one). To do so:\n",
    "\n",
    "    * Create a mask to detect pixel which are below 10% \n",
    "    * With the above mask, set the affected pixels to the 10% 'low value'.\n",
    "    * Do the same for value above 90%\n",
    "    * Create the mask of all the modify pixel\n",
    "\n",
    "3. Store the source, the mask of changed pixels and the result inside ``process.h5``, as below.\n",
    "\n",
    "   ![Output file structure](images/exercise-result.png)\n",
    "\n",
    "4. Load ``process.h5`` and list the root content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load data/medipix.edf\n",
    "# ...\n",
    "\n",
    "# Process the data\n",
    "# ...\n",
    "\n",
    "# Save data into a new file (process.h5)\n",
    "# ...\n",
    "\n",
    "# Load process.h5 and list the root content\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data/medipix.edf\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.load_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# process data\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.process_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.save_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# list root\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.list_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# result\n",
    "import exercicesolution\n",
    "raw_data, proc_data, mask = exercicesolution.solution(\"data/medipix.edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imshow(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imshow(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Preconized libraries according to the use case and the file format.\n",
    "\n",
    "| Formats              | Read            | Write |\n",
    "|----------------------|-----------------|-------|\n",
    "| HDF5                 | silx/h5py       | h5py  |\n",
    "| Specfile             | silx            |       |\n",
    "| EDF                  | silx/fabio      | fabio |\n",
    "| Other raster formats | silx/fabio      | fabio |"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
