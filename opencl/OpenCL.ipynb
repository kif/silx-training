{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPU programming with PyOpenCL\n",
    "\n",
    "Layout\n",
    "\n",
    "1. Modern CPU are not that different from GPU\n",
    "2. Introduction to OpenCL \n",
    "3. Comparison with julia, numba, numba-cuda, pycuda, cupy\n",
    "4. First kernel \n",
    "5. Parallel programming design patterns\n",
    "6. Metaprogramming in PyOpenCL\n",
    "7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![CPU vs GPU](cpu_gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to OpenCL\n",
    "* Vendor & platform neutral parallel programming language to address CPU, GPU, FPGA and other types of accelerators. \n",
    "* Initated by Apple in 2008 and managed by the Kronos group, currently at version 3.0. \n",
    "\n",
    "* There is a strict separation between host code and device code (called kernels).\n",
    "* Kernels are written in a subset of C99 (without pointers) and compiled at _runtime_.\n",
    "\n",
    "* Bindings for host code, initially in C are available for all programming languages: Python.\n",
    "\n",
    "* Sycl (Intel) is the descendent of OpenCL where C++ host code can be mixed with kernels (à la CUDA).\n",
    "\n",
    "* There is a direct mapping between CUDA kernels and OpenCL (version 1.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition:\n",
    "\n",
    "In manycore-devices programming, a `kernel` is the core part of the computational code, without the outer loop, and runs in parallel on a device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "size = 1000\n",
    "a = numpy.random.random(size)\n",
    "b = numpy.random.random(size)\n",
    "res = numpy.empty_like(a)\n",
    "\n",
    "for idx in range(size):\n",
    "    res[idx] = a[idx] + b[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The kernel is `res[i] = a[i] + b[i]`.\n",
    "\n",
    "The size of the problem, `size`, is precised at runtime, usually in host code.\n",
    "\n",
    "The position of the working element `idx` is obtained via specific API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison with other programming languages ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Julia\n",
    "\n",
    "* Julia's strength resides in the just-in-time compilation with strong dynamic typing.\n",
    "* Julia has a strong support for GPU programming.\n",
    "* Since OpenCL is compiled at runtime, so similar to just-in-time, but with static types (without templates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Numba\n",
    "\n",
    "* Numba's strength resides in the just-in-time compilation with strong dynamic typing ... in Python.\n",
    "* Not all Python is supported\n",
    "* Heavy LLVM dependency ... but not worse than the OpenCL driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Numba-cuda\n",
    "\n",
    "* Same as `numba` but targeting Nvidia's GPU\n",
    "* Limited to element-wise, stencil and reduction kernels\n",
    "* It is strange to write kernels in Python ...\n",
    "*   ... Python becomes statically typed !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cupy\n",
    "\n",
    "* Cuda interface with `numpy` like functions (same signature)\n",
    "* automatic management of memory...\n",
    "* ... hence huge overhead due to memory transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PyCuda\n",
    "* Same author and philosophy as PyOpenCL: Expose everything, then add some metaprogramming sugar\n",
    "* Explicit kernel in C(++) and explicit memory management\n",
    "* Runtime compilation (unlike Cuda which is ahead of time)\n",
    "* Interfaces to advanced libraries like `cublas`, `cufft`, ... via `scikit-cuda`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyOpenCL\n",
    "\n",
    "* Expose the full OpenCL API\n",
    "* Explicit kernels in C99 \n",
    "* Explicit memory management\n",
    "* Explicit device and context management\n",
    "* Asynchronous execution with queues and events\n",
    "* Runtime compilation (since the driver contains the compiler)\n",
    "* Many metaprogramming features to expose advanced parallel algorithms\n",
    "* Integration in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose platform:\n",
      "[0] <pyopencl.Platform 'Portable Computing Language' at 0x7f0ef1caf008>\n",
      "[1] <pyopencl.Platform 'NVIDIA CUDA' at 0x2114490>\n",
      "[2] <pyopencl.Platform 'AMD Accelerated Parallel Processing' at 0x7f0ee9c50f30>\n",
      "[3] <pyopencl.Platform 'Intel(R) OpenCL' at 0x2122b40>\n",
      "Choice [0]:2\n",
      "Set the environment variable PYOPENCL_CTX='2' to avoid being asked again.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "import pyopencl.array as cla\n",
    "\n",
    "ctx = cl.create_some_context(interactive=True)\n",
    "queue = cl.CommandQueue(ctx, \n",
    "                         properties=cl.command_queue_properties.PROFILING_ENABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Array creation\n",
    "shape = (10000000,)\n",
    "a = np.random.random(shape).astype(np.float32)\n",
    "b = np.random.random(shape).astype(np.float32)\n",
    "\n",
    "#Reference result\n",
    "ref = a+b \n",
    "\n",
    "# Send data to the device and prepare output buffer\n",
    "a_d = cla.to_device(queue, a)\n",
    "b_d = cla.to_device(queue, b)\n",
    "res_d = cla.empty_like(a_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "kernel void add(global float* a,\n",
    "                global float* b,\n",
    "                global float* res){\n",
    "    \n",
    "    int idx = get_global_id(0);\n",
    "    res[idx] = a[idx] + b[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyopencl._cl.Event object at 0x7f0ef50cb8b0>\n"
     ]
    }
   ],
   "source": [
    "evt = add(queue, shape, None,\n",
    "          a_d.data, b_d.data, res_d.data)\n",
    "print(evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(ref, res_d.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time on GPU: 1.690 ms\n",
      "Execution time on CPU:\n",
      "7.48 ms ± 76.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Execution time on GPU: {1e-6*(evt.profile.end-evt.profile.start):.3f} ms\\nExecution time on CPU:\")\n",
    "\n",
    "%timeit a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel programming design patterns\n",
    "\n",
    "* Map: Pixel wise operations like the `add` kernel.\n",
    "* Gather: Stencil like operation, for example convolutions.\n",
    "* Scatter: write at variable position, requires atomic operations.\n",
    "* Reduction: perform the sum for all elements in an array.\n",
    "* Scan: Perform the `cumsum`, sum of all previous elements, used in compactions.\n",
    "* Sort: Bitonic sort is one example of parallel sort.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beside `Map`, all kernel require dozens to hundreeds of lines of code to implement the algorithm!\n",
    "\n",
    "PyOpenCL provides templates to all those algorithm making programmer's life simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating `Map` kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyopencl.elementwise import ElementwiseKernel\n",
    "\n",
    "t_add = ElementwiseKernel(ctx, \n",
    "                          arguments=\"float* a, float* b, float* res\", \n",
    "                          operation=\"res[i] = a[i] + b[i]\")\n",
    "\n",
    "#reset the destination array:\n",
    "res_d.fill(0)\n",
    "\n",
    "t_add(a_d, b_d, res_d)\n",
    "np.allclose(ref, res_d.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 µs, sys: 67 µs, total: 133 µs\n",
      "Wall time: 138 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyopencl.array.Array"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even more trivial:\n",
    "c_d = a_d + b_d\n",
    "\n",
    "%time a_d + b_d\n",
    "\n",
    "type(c_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduction kernel, like the sum of all elements in an array:\n",
    "![Reduction kernel](kernel-code-sum-reduction.png)\n",
    "\n",
    "One can use it to perform the scalar product ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product implemented as reduction kernel\n",
    "from pyopencl.reduction import ReductionKernel\n",
    "dot = ReductionKernel(ctx, \n",
    "                      dtype_out=np.float32, \n",
    "                      neutral=\"0\",\n",
    "                      reduce_expr=\"a + b\", \n",
    "                      map_expr=\"a[i] * b[i]\",\n",
    "                      arguments=\"__global float* a, __global float* b\")\n",
    "\n",
    "np.isclose(dot(a_d, b_d).get(), np.dot(a,b.astype(np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699 µs ± 17.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.61 ms ± 253 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dot(a_d, b_d).get()\n",
    "a64 = a.astype(np.float64)\n",
    "b64 = b.astype(np.float64)\n",
    "%timeit np.dot(a64,b64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scan kernel, sum of all previous elements from the array:\n",
    "![Scan kernel](300px-Prefix_sum_16.svg.png)\n",
    "Their typical application is in compaction or in compression algorithms. \n",
    "In `numpy` this is implemented by the `cumsum` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyopencl.scan import GenericScanKernel\n",
    "cumsum = GenericScanKernel(ctx, \n",
    "                           np.float32,\n",
    "                           arguments=\"__global float* ary, __global float* out\",\n",
    "                           input_expr=\"ary[i]\",\n",
    "                           scan_expr=\"a+b\", \n",
    "                           neutral=\"0\",\n",
    "                           output_statement=\"out[i] = item;\")\n",
    "\n",
    "cumsum(a_d, res_d)\n",
    "np.allclose(res_d.get(),np.cumsum(a64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 ms ± 30.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "40 ms ± 99.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cumsum(a_d, res_d).wait()\n",
    "%timeit np.cumsum(a64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Scan` kernels are at the core of **memory compaction** but also of **compression** algorithm, as demonstrated in:\n",
    "https://doi.org/10.1107/S1600577518000607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "* PyOpenCL is an interesting Python binding for doing GPU programming:\n",
    "  - Platform independant (i.e. without Nvidia lock-in)\n",
    "  - Comfort of Python and Jupyter\n",
    "  - Full control of execution and memory management\n",
    "  - Scales to larger projects (`silx`, `pyFAI`)\n",
    "  - Can also exploit manycore CPUs\n",
    "  - Great for continuous integration (with Intel driver)\n",
    "  - Fully open source driver exists (PortableCL)\n",
    "* Parallel programming design pattern are well documented\n",
    "  - Knowing them allows to address performance issues with the proper tool\n",
    "  - Most of them are already implemented into PyOpenCL/PyCuda via metaprogramming\n",
    "* Kudos to Andreas Kloeckner, author of PyOpenCL"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
